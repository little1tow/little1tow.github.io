<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Neural Architectures for Fine-grained Entity Type Classification]]></title>
    <url>%2F2018%2F06%2F20%2F2018-06-20%2F</url>
    <content type="text"><![CDATA[Title: Neural Architectures for Fine-grained Entity Type Classification Author: Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, Sebastian Riedel Link: https://arxiv.org/pdf/1606.01341 包含文章 An Attentive Neural Architecture for Fine-grained Entity Type Classification Neural Architectures for Fine-grained Entity Type Classification 这两篇文章是同一组作者做的，相关内容也比较类似，因此放到一起进行介绍 问题定义问题Named Entity Typing (NET)主要是为实体表示（entity mention）标记其类别的一个任务，输入一般是一个句子，包含情境和实体表示，一般用$[C_{-s}, C_{-s+1}, …, C_{-1}][w_1, w_2, …, w_n][C_1, C_2, …, C_s]$表示，输出的结果就是实体表示的类别，和传统的分类任务不同的地方在于，实体表示有可能是属于多个类别的（如果分类粒度比较细的话），而且这些类别之间也存在包含关系之类的，这点就比较复杂了，目前比较常用的做法是首先根据输出概率，选择一个最大的作为entity mention的类别（保证至少有一个类别），然后设定阈值，大于阈值的都可以认为是该entity mention的类别。 评价标准一般使用以下三个作为实验效果的评价标准： strict $$Precision = Recall = \frac{1}{N}\sum_{i=1}^{N}\delta(\hat{T}_i = T_i), \quad \tag{0.1}$$ loose macro $$Precision = \frac{1}{N}\sum_{i=1}^N\frac{|\hat{T}_i\cap T_i|}{|\hat{T}_i|}, \quad \tag{0.2}$$ $$Recall = \frac{1}{N}\sum_{i=1}^N\frac{|\hat{T}_i\cap T_i|}{T_i}, \quad \tag{0.3}$$ loose micro $$Precision = \frac{\sum_{i=1}^N|\hat{T}_i \cap T_i|}{\sum_{i=1}^{N}|\hat{T}_i|}, \quad \tag{0.4}$$ $$ Recall= \frac{\sum_{i=1}^N|\hat{T}_i \cap T_i|}{\sum_{i=1}^{N}|T_i|}, \quad \tag{0.5}$$ 模型框架 该框架展示了基于神经网络的NET模型的基本结构，在本文中利用上图对该 框架进行简单介绍 Entity Mention表示首先得到每个词的词向量，将entity mention中的所有词向量做平均，得到entity mention的表示，考虑到entity mention一般不是很长，所以该方法简单有效，当然也可以使用RNN之类的方法进行表示， $$v_m = \frac{1}{M}\sum_{i=1}^Mu(m_i), \quad \tag{1}$$ 其中，u就是将每个词表示为它的词向量表示。 context表示很容易理解，entity mention的语义是十分依赖其所存在的情境的，如果是在一个句子中的话，那么该情境信息就是上下文的词，在本文中，作者通过三种方法来处理这些词： 和entity mention表示类似，采用和平均的方法 $$v_c = \frac{1}{C}\sum_{i=1}^{C}[u(l_i), u(r_i)], \quad \tag{2.1}$$ 考虑到情境信息是一个序列关系，因此也可以采用LSTM来处理 $$h_i, s_i = lstm(u_i, h_{i-1}, s_{i-1}), \\ v_c = [\overrightarrow{h_C^l}, \overleftarrow{h_1^r}] \quad \tag{2.2}$$ 单纯那最后一个状态进行拼接并不能有效利用LSTM中的信息，因此注意力机制派上了用场，这里采用了得是一种类似于self-attention的方法： $$e_i^l = tanh(W_e[\overrightarrow{h_i^l}, \overleftarrow{h_i^l}]), \quad \tag{2.31}$$ $$\widetilde{a}_i^l = exp(W_ae_i^l) , \quad \tag{2.32} $$ $$ a_i^l = softmax(\widetilde{a}_i^l) \quad \tag{2.33}$$ $$v_c = \sum_{i=1}^{C}a_i^l[\overrightarrow{h_i^l}, \overleftarrow{h_i^l}] + a_i^r[\overrightarrow{h_i^r}, \overleftarrow{h_i^r}], \quad \tag{2.34}$$ 从这里我们可以看出是由各自的输入决定各自的权重，但在最后计算整体的权重正规化时同时考虑了左侧和右侧的权重，在这里将这两部分同时考虑，最后得到一个加权和作为最后的context表示。 分类分别得到entity mention和context的表示之后，普遍的做法是直接将这两部分拼接起来，然后进行逻辑回归， $$y = \frac{1}{1 + exp(-W_y[v_m, v_c])}, \quad \tag{3.1}$$ 有了预测结果，考虑到这是一个分类任务，那么就可以使用交叉熵作为损失函数， $$L(y, t) = \sum_{k=1}^K-t_klog(y_k) - (1-t_k)log(1-y_k), \quad \tag{3.2}$$ 在这个损失函数中，K表示所有的类别数，t是预测出来的二值向量，即在每个类别上都要做一个二分类，根据之前的介绍，这部分需要这么做，因为他的分类结果是不一定的。 这就是基于神经网络的NET模型的大体框架，从我看到的几篇文章中，基本上都是用了这样的框架，只是在处理细节上略有不同， 额外部分本文题目的这篇文章可以认为是作者对前一篇工作的改进，具体改进部分有两点： 人工特征信息考虑到有些人工特征信息是十分重要的 ，但如果直接让模型去学习，需要花费很大的精力，现在整个神经网络结构也趋向于加入一些简单的人工特征信息，本文也是考虑到这些信息，具体如下图： 这些特征基本上将entity mention的一些语义信息，主题信息等考虑了进去，相当于增加了很多先验知识，这个还是十分有用的，那如何加这些信息呢？作者首先用0，1向量表示这些特征信息，然后将其映射到低维空间，最后得到人工特征的向量表示 $$v_f = W_ff(m). \quad \tag{4.1}$$ 然后将其加入到公式(3.1)中的拼接向量，变为如下形式： $$y = \frac{1}{1 + exp(-W_y[v_m, v_c, v_f])}, \quad \tag{4.2}$$ 这个还是很有特点的，一般我们都是将这些特征信息加入到最初的输入中，这样丰富了输入的信息，整个模型也可以更好地利用这些信息，这个方法个人感觉这些特征信息利用的不够充分。 分层标签编码这是本文另一个很有意思的地方，首先，作为分类目标，其实考虑到他的类别表示，也是可以使用词向量进行编码的，而且这么做有一个好处，一些不常见的标签可以通过这种方式找到离他比较近的语义空间向量，从而能够更准确的进行分类，作者也是考虑到这样的信息，同时分类内容比较细的话，标签信息之间具有包含关系，因此，作者使用0，1向量表示类别信息，同时包含high-level和low-level的标签，然后对它们进行混合编码，而不是每个类别一个向量表示，具体可以从下图感受： 然后就得到了类别表示矩阵$W_y$，巧妙地地方在于，作者使用这个矩阵表示作为公式（4.2）分类层的参数，从而很好的将类别信息引入到了分类过程中，这样模型在分类的的时候就能获取更多的信息了，这个信息融合方式还是很有意思的。 实验结果照例贴出最后的结果图，作者的方法虽然简单，但还是很有效的 个人总结这篇文章展示了基于神经网络的NET模型的基本结构，包括对context的处理，对entity mention的处理，attention的使用，人工特征的添加等，是一篇很不错的文章，同时该方法也有很多地方值得改进，例如人工信息的利用，注意力机制，对文本信息的处理等，感觉还是有很多地方可以改进的，值得思考↖(^ω^)↗]]></content>
      <categories>
        <category>paper_reading</category>
      </categories>
      <tags>
        <tag>NET</tag>
        <tag>representation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering]]></title>
    <url>%2F2018%2F06%2F17%2Fpaper-reproduction%2F</url>
    <content type="text"><![CDATA[Title: Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering Author: Wuwei Lan, Wei Xu Link: https://arxiv.org/pdf/1806.04330 介绍这篇文章是COLING 2018的beat reproduction paper，文章主要对现有的做句子对任务的最好的几个模型进行了重现，并且作者实现出来的效果和原文章声称的效果相差不多，这点还是很厉害的，而且作者对语义理解的集中任务也做了相关梳理，文章简单易读，还是很值得一看的。 任务句子对建模是NLP，NLU中比较基础，并扮演着重要角色的任务，主要集中在语义理解，语义交互上，也是我自己的一个研究方向，大致有这几类任务 Semantic Textual Similarity (STS) ：判断两个句子的语义相似程度（measureing the degree of equivalence in the underlying semantics of paired snippets of text） Natural Language Inference (NLI) ：也叫Recognizing Textual Entailment(RTE)，判断两个句子在语义上是否存在推断关系，相对任务一更复杂一些，不仅仅是考虑相似，而且也考虑了推理。 Paraphrase Identification (PI) ：判断两个句子是否表达同样的意思（identifing whether two sentences express the same meaning） Question Answering (QA) ：主要是指选择出来最符合问题的答案，是在给定的答案中进行选择，而不是生成 Machine Comprehension (MC) ：判断一个句子和一个段落之间的关系，从大段落中找出存在答案的小段落，对比的两个内容更加复杂一些。 模型有了任务，作者选取了集中目前情况下最好的模型，因为原文中每个模型可能只针对了某些任务进行了很多优化，那这些模型是否真的有效呢，作者考虑这些模型在所有的任务上进行比较，在介绍模型之前，作者首先介绍了句子对建模的一般框架： 一般框架 输入层：适用预训练或者参与训练的词向量对输入中的每个词进行向量表示，比较有名的Word2Vec，GloVe，也可以使用子序列的方法，例如character-level embedding 情境编码层：将句子所处的情境信息编码表示，从而更好的理解目标句子的语义，常用的例如CNN, HighWay Network等，如果是句子语义表示的方法，一般到这里就结束了，接下来会根据具体的任务直接使用这一层得到语义表示 交互和注意力层：该层是可选的，句子语义表示有时候也会用到，但更多的是词匹配方法用到的，通过注意力机制建模两个句子在词层面的匹配对齐关系，从而在更细粒度上进行句子对建模，个人认为句子语义表示也会用到这些，只是句子语义表示最后会得到一个语义表示的向量，而词匹配的方法不一定得到句子语义的向量 输出分类层：根据不同的任务，使用CNN，LSTM，MLP等进行分类判断。 下图展示了一些句子语义表示的模型的基本框架： 有了这个一般的框架，接下来作者选取了集中目前最好的模型进行重现 模型选择 InferSent[1]：BiLSTM+max-pooling SSE[2]：如图1，和InferSent比较类似 DecAtt[3]：词匹配模型的代表，利用注意力机制得到句子1中的每个词和句子2中的所有词的紧密程度，然后用句子2中的所有词的隐层状态，做加权和表示句子1中的每个词 ESIM[4]：考虑了一些词本身的特征信息，和DecAtt比较类似 PWIM[5]：在得到每个词的隐层状态之后，通过不同的相似度计算方法得到词对之间相似关系，最后利用CNN进行分类。 数据：为了更好的展示每个数据的情况，在这里直接用下图展示作者使用到的数据集： 结果直接上结果，上图是原文章中的结果，下图是作者重现的结果 从结果上看，作者实现的效果还是很厉害的，基本上跟原文章声明的不相上下，当然由于不是针对特定任务进行特别优化，所有效果还是有一点点差的，但基本上可以认为是实现了原来的效果，而且作者也发现了一些有意思的现象，例如：表现最好的就是ESIM，个人感觉这里面加入了很多次本身的一些信息，例如近义词，反义词，上下位信息等，这些信息其实对句子语义理解十分重要。 以上就是这篇文章的整体介绍，作者完整实现了这些方法，并在不同的数据集上进行验证，工作量还是很大的，而且对句子对建模进行了比较完整的介绍，还是很有意思的。♪(＾∀＾●)ﾉ 引用[1]：Supervised learning of universal sentence representations from natural language inference data [2]：Shortcut-stacked sentence encoders for multi-domain inference [3]：A decomposable attention model for natural language inference [4]：Enhanced LSTM for natural language inference [5]：Pairwise word interaction modeling with deep neural networks for semantic similarity measurement]]></content>
      <categories>
        <category>paper_reading</category>
      </categories>
      <tags>
        <tag>natural language inference</tag>
        <tag>natural language understanding</tag>
        <tag>semantic similarity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Improving Neural Fine-Grained Entity Typing with Knowledge Attention]]></title>
    <url>%2F2018%2F06%2F17%2Fpaper-NET%2F</url>
    <content type="text"><![CDATA[Title: Improving Neural Fine-Grained Entity Typing with Knowledge Attention Author: Ji Xin, Yankai Lin, Zhiyuan Liu, Maosong Sun Link: http://nlp.csai.tsinghua.edu.cn/~lzy/publications/aaai2018_entitytyping.pdf 任务本文针对的是Named Entity Typing(NET)任务，该任务主要是判断一个实体表示(entity mention)的类别，粒度的分类可以分为：person, location, organization，others。细粒度的分类根据不同的数据有着不同的分类情况，例如：FIGER dataset 有112类；也可以根据需要自定义类别个数进行分类，例如：从DBpedia中抽取22类。 该任务也是一个比较基础的任务，可以为问答，对话，语义理解等提供一些辅助理解的信息，而且和语义理解类似，同一个实体表示在不同的情境(context)下的类别是不一样，同样需要考虑情境信息。 现状传统的方法大多集中在特征抽取，例如选取多种特征抽取的方法，例如pattern-based extractor，Knowledge base，mention-based extractor等，利用这些不同的方法得到实体表示的不同类别候选，然后利用词义消歧（word sense disambiguation ）的方法选择最合适的类别。这里边最受关注的地方在于知识图谱的运用，知识图谱将很多先验知识及关系整合到图中，尤其是NET中的细粒度分类， 单纯利用神经网络来做是不行的，很多类别的样本都很少，而知识图谱可以提供必要的帮助。 神经网络的方法目前也慢慢多起来，神经网络在提取特征阶段已经被证明非常有效，因此在该任务上神经网络+知识图谱应该是一种非常有效的方法，而本文就是这样的一种方法。 模型 模型整体看起来是比较简单的，而本文主要是为了解决NET中的两个问题： 在实体表示分类时需要考虑其所存在的情境，而这点是被忽略的； 虽然知识图谱已经很早就被用到了NET中，但是知识图谱中实体之间的关系，并没有得到很好的利用。 因此，作者提出了如上的模型，具体如下： 首先适用预训练的模型对所有的词进行标识 ，得到每个词的词向量， 实体表示的向量表达考虑到实体表示的短语是比较短的，大多是都是一个或者两个词，因此直接将每个词的向量相加，取均值，得到实体表示的向量表达： $$m = \frac{1}{n_m}\sum_{i=1}^{n_m}m_i \quad \tag{1}$$ 情境信息的表示为了更好的表示中心词的情境，例如中心词所存在的句子，作者使用BiLSTM分别整合左边和右边的情境信息，并利用注意力机制整合他们在LSTM中的隐层状态，最后整合为一个情境表示向量： $$c = \frac{\sum_{i=1}^{L}(a_i^l[\overrightarrow{h_i^l}, \overleftarrow{h_i^l}]+a_i^r[\overrightarrow{h_i^r}, \overleftarrow{h_i^r}]) }{\sum_{i=1}^{L}a_i^l+a_i^r} \quad \tag{2}$$ 这个公式还是很好理解的，将每个状态的两个方向表示拼接起来，乘以对应的权重，然后做加权和，最后再做平均，就得到了情境信息的向量表示，那么接下来就是如何表示情境信息的权重了。 注意力机制在这里作者考虑了很多不同的注意力机制， 语义注意力机制，就是考虑情境信息的权重，由自身的语义信息来计算所占的比重，类似于self-attention： $$a_i^{SA} = \sigma(W_{s1}tanh(W_{s2}[\overrightarrow{h}_i, \overleftarrow{h}_i])). \quad \tag{3.1}$$ 因为我们需要考虑的是实体表示的类别，那么也可以计算实体表示所关注的重点在什么地方，也就是基础版的attention： $$a_i^{MA} = \sigma(mtanh(W_{MA}[\overrightarrow{h}_i, \overleftarrow{h}_i])). \quad \tag{3.2}$$ 正如之前问题里提到的那样，知识图谱不仅包含了实体，同时还有实体之间的关系信息，这点也是十分重要的，因此作者使用了知识图谱中表示关系的方法TransE来处理知识图谱中的关系表示，并将其应用到注意力机制中： $$a_i^{KA} = \sigma(etanh(W_{KA}[\overrightarrow{h}_i, \overleftarrow{h}_i])), \quad \tag{3.3}$$ 这其中$e$就是实体的embedding表示， 类别预测有了实体表示的向量和情境信息的向量，最后就是一个分类了，作者将得到的两个向量表示拼接起来，然后通过一个两层的MLP进行分类，得到最后的结果： $$y = \sigma(W_{y1}tanh(W_{y2}[m, c])), \quad \tag{4}$$ 最后使用二分类的交叉熵作为损失函数，因为每一个实体表示的类别可能不止一个，这可能是一个多分类问题，因此作者在每个类别上都是用了二分类的交叉熵，最后将所有的全都加起来作为最终的损失函数。 实验结果照例，还是贴一个最后的结果图，因为使用了神经网络将情境信息融入到了实体分类中，可以更好地分类出在特定情境下的类别，也就更符合实际情况，从而获得更好的效果，当然将知识图谱中的关系信息也考虑进去也是本文的另一个创新点，这点还是很有意思的。 个人评价这是我接触NET之后读的第一篇利用神经网络来做的方法，对比那些利用不同的特征抽取方法来对实体表示进行分类，神经网络在抽取特征方面确实是省了不少的力气，而且整个模型简单有效，这就是一种非常好的方法。当然，该方法相对来说还是比较简单的，例如在情境信息的表示，情境信息和知识图谱的融合，相互辅助等方面还有很多可以做的，而且这个问题对语义理解也是一个很重要的方面，很值得学习研究。 以上就是这篇文章的整体介绍，NET还是很有意思的，搞起来搞起来♪(＾∀＾●)ﾉ]]></content>
      <categories>
        <category>paper_reading</category>
      </categories>
      <tags>
        <tag>NET</tag>
        <tag>KB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sentence Modeling via Multiple Word Embeddings and Multi-level Comparison for Semantic Textual Similarity]]></title>
    <url>%2F2018%2F06%2F07%2Fpaper-20180607%2F</url>
    <content type="text"><![CDATA[这篇文章主要是集中在对词的多方面语义表示，在此基础上实现对句子语义的准确理解，并在多个句子语义理解相关任务上取得了不错的表现。 Different word embedding models capture different aspects of linguistic properities. 模型结构不同的词向量模型获取的是不同的语义信息，例如：BOW的上下文反映的是词的domain信息（scientist and research），基于解释关系的上下文反映的是词的语义相似度信息（boy and kid），因此作者提出来通过选取不同的上下文来获取词在不同方面的语义信息，具体作者选了 word2vec：在Google News上进行训练的 fastText：在wikipedia上训练的 GloVe：选择的是300d的词向量，训练数据为Common Crawl Baroni：使用的是context-predict的方法，使用的训练数据为English Wikipedia+British National Corpus SL999：使用的训练数据为paraphrase database PPDB，然后再SimLex-999上微调 以下是模型的整体框架图： 输入处理作者使用K个预训练的词向量表示每个词，然后将这些结果拼接起来，作为每个词的最终表示 $$e_w^{concat} = e_w^1\oplus e_w^2 \oplus … \oplus e_w^K \tag{1}$$ 卷积网络为了学习到一个可以表示每个词的multi-aspect的语义，作者使用了一个卷积网络的结构，有H个卷积核，每个卷积核可以通过一个线性变换表示，具体如下： $$e_w^{r_i} = \sigma(e_w^{concat}r_i^T + b_{r_i}) \\ e_w^{multi} = [e_w^{r_1}, e_w^{r_2}, … , e_w^{r_H}] \tag{2}$$ 句子建模使用卷积网络的好处是参数共享，可以学习到局部特征，但是卷积神经网络有一个问题，在处理序列数据的时候容易丢失序列信息，因此作者在这里使用了LSTM来处理，从而保证了序列信息的保留，具体如下，作者首先在句子中所有词的相同纬度上进行max-pooling，获取到最有价值的信息，同时通过LSTM实现对整个句子的建模，然后选取最后一个状态作为句子的语义向量表示，最后将max-pooling的结果和LSTM的最后一个状态进行拼接，从而得到句子的语义表示，具体如下： $$e_s^{max}[i] = max(e_{w_1}^{multi}[i], e_{w_2}^{multi}[i], …, e_{w_n}^{multi}[i]) \\ e_s^{lstm} = LSTM(e_w^{multi})[-1] \\ e_s = e_s^{max} \oplus e_s^{lstm} \tag{3}$$ 多尺度比较有了每个句子的语义表示，根据任务的不同，就可以进行比较了，作者在这里选取了三种比较方式 word-word similarity： $$A_{ij} = \frac{s_1^{multi}[i] \cdot s_2^{multi}[j]}{\left|s_1^{multi}[i]\right|\left|s_2^{multi}[j]\right|} \\ sim^{word} = \sigma(W^{word}g(A)+b^{word})\tag{4}$$ 这个公式中，g表示的是讲一个矩阵展平为向量的函数，这是词与词之间的相似度比较 sentence-sentence comparison: Cosine Similarity: $$d_{cosine}=\frac{e_{s_1}\cdot e_{s_2}}{\left|e_{s_1}\right|\left|e_{s_2}\right|} \tag{5.1}$$ Multiplication vector &amp; Absolute difference: $$d_{mul}=e_{s_1}\odot e_{s_2} \\ d_{abs} = |e_{s_1} - e_{s_2}| \tag{5.2}$$ Neural difference: $$x = e_{s_1}\oplus e_{s_2} \\ d_{neu} = W^{neu}x + b^{neu} \tag{5.3}$$ 最后将这些不同的结果拼接起来，做一个线性变换 $$d^{sent} = d_{cosine}\oplus d_{mul}\oplus d_{abs} \oplus d_{neu} \\ sim^{sent} = \sigma(W^{sent}d^{sent} + b^{sent}) \tag{5.4}$$ word-sentence comparison: 在这部分就是让句子1的语义表示和句子2的每个词的语义表示进行比较，然后将比较结果矩阵展平，通过线性变化，得到这一阶段的相似度值 $$e_{s_1}^{ws}[i] = e_{s_1} \oplus s_2^{multi}[i] \\ sim_{s_1}^{ws}[i] = \sigma(W^{ws}e_{s_1}^{ws}[i]+b^{ws}) \\ sim^{ws}=\sigma(W^{ws^{‘}}[g(sim_{s_1}^{ws})\oplus g(sim_{s_2}^{ws})] + b^{ws^{‘}}) \tag{6}$$ 最后进行分类在这一阶段，作者将三种不同的相似度信息进行拼接，然后通过线性变换，最后softmax进行分类 $$sim = sim^{word} \oplus sim^{sent} \oplus sim^{ws} \\ h_s = \sigma(W^{l1}sim+b^{l1}) \\ \hat{y} = softmax(w^{l2}h_s+b^{l2}) \tag{7}$$ 最后的输出根据不同任务的不同有不同的修改。以上就是整个模型的过程。 任务作者是在比较两个句子之间的相似度，因此作者在不同的几个任务上进行了比较，分别是1）RTE: SICK数据集；2）STS：STSB和SICK数据集，这部分需要吐槽的一点是在RTE任务上居然没有使用SNLI这个大规模数据集，可能是效果表现不好，个人感觉 效果这里就贴一张结果图来展示作者实验的效果吧 个人评价首先作者的这个idea还是很有意思的，我在16年的一篇工作也使用了同样的一个idea，基本的motivation也是相同的，只是他在多个不同的任务上进行验证，同时将多方面的词表示扩展到了句子的语义表示上，我那个工作只是在词级别上的一个分析，同时只考虑了Recognizing lexical entailment这个任务，这点是我的不足。但这篇文章在句子语义建模上有些简单，因此个人感觉他的句子语义表示向量的效果可能不是很好，但作者在最后的相似度上考虑了词对词，词对句子，句子对句子三个方面，这个还是很有意思的，这点值得学习一下，考虑多方面语义时不仅要考虑输入上的多方面，在比较上也可以进行多方面的比较。 以上就是这篇文章的整体介绍，介绍完感觉有必要介绍一下我自己的工作，下次介绍♪(＾∀＾●)ﾉ]]></content>
      <categories>
        <category>paper_reading</category>
      </categories>
      <tags>
        <tag>word semantic</tag>
        <tag>natural language understanding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文投稿有感]]></title>
    <url>%2F2018%2F06%2F06%2FAfter-Writing%2F</url>
    <content type="text"><![CDATA[刚刚忙完了论文的投稿，虽说也写了几篇文章，但是进入依然十分缓慢，这次记录下自己在这次投稿过程中的感想，与君共勉 Title标题是整篇文章的开头，一定要清晰直接的介绍出自己的工作的卖点，不能使用那些会造成误解的词，否则开头就容易把人带偏，就像我这次，开始使用的是Hierarchical，后来改成了Multi-Level，虽然前者也有后者的意思，但是前者更偏向于树形结构，有等级从属关系，这样就容易给人造成误解， 所以起一个好的名字是十分有必要的。 Abstract摘要是一篇文章最精炼的部分了，从我的感觉来看，可以分成以下的形式： 第一句话：介绍要研究的问题 第二句话：当前工作有什么难题 第三句话：我们提出了什么方法，直接点明 第四句话：我们方法的卖点 第五句话：效果是怎么样的，如果有具体数据支撑就更好了 基本上可以用这些来表示，需要注意的是在这里提出的问题，突出的卖点是整篇文章的核心，在之后的部分都要有具体的支撑。摘要部分是最精炼的部分，也展示了文章最突出的地方，能让读者看了之后就明白作者的想法。 Introduction这部分我先举一个我在整个文章过程中的修改历程： 介绍问题-&gt;当前的研究现状-&gt;问题1，我们需要怎么做-&gt;正好有一些别的任务证明这么做是有效的-&gt;问题2，我们需要怎么做-&gt;总结，提出我们的方法，简单介绍-&gt;写贡献点 介绍问题-&gt;分类介绍当前研究现状及特点-&gt;存在的问题2，我们应该怎么做-&gt;问题1，我们应该怎么做-&gt;正好有一些别的任务证明这么做有效&gt;总结，提出我们的方法，简单介绍-&gt;写贡献点 介绍问题-&gt;分类介绍当前研究现状及特点-&gt;问题1，我们需要怎么做-&gt;当我们这么做时又会碰到什么问题-&gt;问题2，需要解决-&gt;基于以上观点，提出我们的方法，简单介绍-&gt;写贡献点 以上就是我在introduction上的大版本修改过程，因为我最初想突出了两个贡献点，而这两个贡献点彼此相差比较大，强硬写到一起，只会逻辑不通，读者也不知道你要表达什么，因此，突出一个重点，围绕这个重点写，所有的工作都是围绕这个重点展开的，这样就会十分清晰，也就容易看懂了。 Related-Work这部分好写，只需要将相关的工作做一介绍，整合他们的特点，指出他们存在的不足即可，需要注意的是相关工作一定要全面，不能丢三落四，否则很容易体现出你的相关调研做的不充分。 但这部分也很难写，如何使用简短的语句将当前的研究现状做一梳理，整合他们的特点，在此基础上提出新的论点，这点其实是非常考验作者功力的，我也在朝这个方向努力中。 还有需要强调一点的，这部分最好全都用一般过去时 Problem Statement &amp; Technical Details首先是问题形式化，这部分相对简单，但要注意符号的运用；同时可以在这里指出在问题中我们要解决什么问题，这个问题和我们的卖点是对应的，这样可以告诉读者，接下来需要注意那些地方，读者也能有重点的看这部分。 接着是技术部分，几点经验教训吧： 符号的定义，最好可以定义一个符号表，保证符号使用规范以及不出错，相关规范可以参考Ian Goodfellow的《深度学习》这本书，同时要注意符号要前后照应，和模型图对应，和各种分析对应；如果使用latex的话，可以定义一个newcommand，这样在后续修改过程中只需要改动这个地方就可以了 ，类似于程序中的头文件。 不能像实验报告那样，只写我们做了什么，我们怎么做，这是绝对不行的，这个也是我要注意的问题 不能像技术文档那样，事无巨细，每一条都有解释，有引文，这样就太啰嗦了，也是不行的 技术部分要详略得当，一些众所周知的内容，可以简单写，涉及到自己工作卖点的部分，一定要清晰解释，为什么要这么做，这么做有什么好处，这么做可以达到一个什么样的效果，然后是具体的做法，这点非常非常重要。 写公式的时候最好可以按照数学的方式写，作为一个计算机专业的学生，有时候写着写着就习惯于按照写程序的方式来写了，须知在程序中的相乘方式和在数学公式中是不一样的，这点需要十分注意。 模型图也要根据自己的卖点突出相应的重点，可以让读者不用看文字就能知道大致的重点是什么，要突出的地方要重点展示出来，同时图中的符号要与文字部分保持一致，保证能够彼此印证，最后图要画的美观大方，不能太繁琐，太复杂。 Experiment这部分我感觉是我的薄弱部分，因为有的时候不知道要做哪些实验来验证，写一个大致的想法吧，首先肯定是当前任务上的整体效果，这点一定要好，否则你就无法体现你模型的贡献了，在这部分，要注意baseline的选择，能够体现出自己工作的特点，同时能够展现当前的state-of-the-art效果，baseline的选择也是一门艺术。其次对自己模型的分析 ，既然提出了某个点十分重要，那肯定需要通过实验来验证它的重要性，并于之前的部分相呼应，因此ablation performance就十分有必要了。接着就是一些参数敏感性实验之类的，总之这部分要十分充实。同时在图标上，表格，折线图，柱状图，饼图等多样化来展现模型的特点，同时也体现出专业性。另外，如果可以做一个case study的话，最好做一个，这样可以更直观的体现出模型的卖点。 也有一点需要强调的，方便书写，这部分用一般现在时会好一些。 Conclusion最后这部分我一般都是做一简单总结，以及对未来工作的一些简单想法，目前还没有学习到好的conclusion写法，还在努力学习中。 还有就是一些小问题了，例如语法，时态，拼写等，这些会经常出现在文章中，因此仔细写，多检查，细心一些是十分有必要的。 以上，就是此次投文章的一些经验总结，希望下次能写出更好的文章出来，↖(^ω^)↗]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>paper-writing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自然语言推理介绍]]></title>
    <url>%2F2018%2F05%2F28%2FNLI-introduce%2F</url>
    <content type="text"><![CDATA[自然语言推理作为自然语言理解的一个重要组成部分，在整个自然语言理解中扮演着重要的角色，接下里我将对自然语言推理的现状做一简单总结，以下内容是我的小组分享的记录版。 自然语言推理简介 自然语言推理主要是判断两个句子（Premise, Hypothesis）或者两个词之间的语义关系，为了保证模型能够集中在语义理解上，该任务最终退化为一个分类任务，目前类别主要是三分类（Entailment，Contradiction，Neutral）。目前对这三类有各种各样的定义，但是我认为这三类的分类依据还是要落在语义理解上，通过语义关系来确定类别。 那为什么要研究自然语言推理呢？简单来讲，机器学习的整个系统可以分为两块，输入，输出。输入要求我们能够输入一个机器能理解的东西，并且能够很好的表现出数据的特点，输出就是根据需要，生成我们需要的结果。也可以说整个机器学习可以分为Input Representation和Output Generation。因此，如何全面的表示输入就变得非常重要了。而自然语言推理是一个分类任务，使用准确率就可以客观有效的评价模型的好坏；这样我们就可以专注于语义理解和语义表示。并且如果这部分做得好的话，例如可以生成很好的句子表示的向量，那么我们就可以将这部分成果轻易迁移到其他任务中，例如对话，问答等。这一切都说明了研究自然语言推理是一个非常重要但是非常有意义的事情。 以下是自然语言推理推理的发展历程 相关数据上一部分对自然语言推理进行了一个大致的介绍。众所周知，数据对模型非常重要，而且深度神经网络也是高度依赖数据的，那么我们来对目前的数据进行一个简单的梳理 这是自然语言推理领域一个非常重要的数据集，相关信息如上图所示，斯坦福大学通过众包的方式生成了这个自然语言推理领域第一个大规模人工标注的数据集，从此自然语言推理进入深度学习时代。 这个数据集和SNLI比较类似，但是它的前提句子来自真实场景的数据，因此在数据上更贴近现实一些，同时，每一条数据包含了类别信息。 这个数据集就更加贴近现实了，前提句和假设句全都来自真实场景，人工的作用放到了对每一条数据打标签，因此数据本身的人工影响就变小了。但是该数据全部来自科学问答，因此在类别上可能略显单一。 该数据集和前边不同的地方在于它是四个前提句对应一个假设句，这样在进行推理分类时，不仅需要考虑前提句和假设句的关系，还需要考虑前提句之间的相关关系，因此推理难度更大，但是这些文本句子全都来自image captioning工作，文本本身人工较少，因此文本质量上可能不如前者。但这也是一个很有意思的方向。 当然，还有很多其他的数据集，例如SICK, Add-one RTE, JOCI等，都是从不同角度对语义理解提出了一个要求。可以说自然语言推理领域目前正是百花齐放，十分繁荣。 一些方法词级别的推理 该方法主要研究的是词级别的推理。有研究表明，词具有不同方面的语义信息，例如book在名词角度可以表示为：书，但在动词角度可以表示为：预定；因此使用单一的向量可能无法有效区分这些内容，因此作者提出利用不同的上下文来获取词在不同角度的语义信息，例如：选取中心词周围的名词来表示它的topic信息，选取中心词周围的动词来表示它的function信息。这样对每个词就都有不同的语义向量表示，然后通过网络结构对相关信息进行拼接，最后考虑到不同的推理关系可能需要的信息时不同的，例如：上下位关系：狗-动物，可能需要的是topic相关的信息，因果关系：攻击-受伤，可能需要的更多的是function的信息，因此作者通过一个门结构计算出每种语义表示对推理关系的影响程度，然后进行加权求和，最后进行分类。并且该方法的可扩展性非常好，从网络结构上看，我们可以增加不同的语义表示，模型的结构和参数规模并不会有太大的提升，这也可以认为是模型的一个优点。 句子级别的推理句子级别的推理可以分为两部分，基于句子语义编码的方法和基于词匹配的方法，前者是首先将一个句子编码为向量，然后分析两个向量之间的关系；后者考虑更多的是词之间的匹配关系，不一定有句子的语义向量表示。 句子语义编码方法 该方法就是句子编码的方法，首先左图展现了这类方法的一般结构，首先通过不同的方法得到每个句子的语义向量表示；在此基础上，对两个向量作拼接，相减，点乘来得到两个语义向量之间的关系，最后通过一个MLP进行分类，右图就是句子编码部分可以采用的方法，例如：通过双向LSTM，得到隐层状态，对隐层状态做max-pooling或者做attention，得到的加权表示就只句子的语义向量表示，最右边的图示利用了CNN的结构，我们都知道CNN是建立输入之间的局部关系，那么作者使用了多层的CNN，通过多层卷积，底层获取的是local部分的信息，那么越往上就可以得到更长范围内的信息，从而对句子语义进行建模，这也是一种很不错的方法。 这个方法是对注意力机制（Attention Mechanism）的一种有效利用，我们可以清晰看出来，作者先对BiLSTM的隐层状态进行mean pooling，在此基础上，利用注意力机制得到句子中那些词对语义表示比较重要，然后对隐层状态进行加权求和，就得到了句子表示的向量，最后就是常用的框架。从该方法中我们也可以看到句子编码模型的基本结构。 词匹配方法 该方法是词匹配方法的一个代表工作，首先作者使用两个LSTM来处理两个句子，并且后一个LSTM的隐层使用前一个LSTM的最后一个状态初始化，在此基础上，作者在求后一个句子的隐层状态时，使用注意力机制考虑前提句子的每一个词的隐层状态，建立词之间的匹配关系，最后利用最后一个隐层状态作为最后分类的依据，在这个方法中我们看到，其实并没有句子语义向量的表示。 该方法在词匹配上进行了更深入的研究，不仅仅是计算他们的匹配程度，而是先求前提句中的每个词和假设句中的所有词之间的attention，并使用假设句中所有词乘以attention分布，来表示前提句中的每个词，这样就使用了对方的语义来表示自己，假设句也是一样。然后将得到的结果和原始的表示作拼接，通过变换，最后求一个和，得到句子的表示进行分类，该方法也是目前比较流行的方法，通过使用对方的语义来表示自己，从而对语义关系进行更好的建模。 问题Accuracy 这是第一个也是很重要的问题，目前在SNLI数据集上，最好的结果已经做到89.3%，那么接下来如何提升准确率呢？这是一个值得思考的问题。 Lexical Knowledge 这个问题很有意思，从数据上考虑，他只是修改了前提句中的一个词得到假设句，对于我们人类来说，进行这样的区分十分容易，但是由于两个句子之间的词的高重合度，模型可能会认为这两个输入是一致的，尤其是不同的两个词属于同一类的时候，他们的词向量表示会更相似。如果数据中有这样的例子，那模型肯定没问题，但如果训练数据中没有这样的例子，但实际上如果模型能够很好地理解语义，这个应该不是问题，而事实上这对模型来说是一个巨大的问题，对于这些词级别的不同，模型该如何去衡量呢？ Annotation Artifacts 这也是一个很有意思的问题，如前边所介绍的，数据集都是使用了人工，尤其是有一些数据集的假设句子全都是人写的，理论上来说人写的句子肯定比模型生成的好，但是人写的句子也有一些特点，例如推理关系是Entailment的时候，可能假设句的一些名词是前提句的上位词（woman-&gt;people），如果是Contradiction的时候，那么假设句中可能就有很多否定词之类的。这些特点其实很好理解，但是如果模型发现了这些特征，那么它甚至可以只用假设句就能进行分类，但结果正确并没有什么用，模型并没有真正理解语义。如图作者进行了一些统计分析，可以看到有一些词和标签是有着紧密联系了，可以直接用这些词进行分类，但这些对语义理解并没有什么帮助。因此如何避免这些情况，准确理解语义也是一个非常重要的研究内容。 最后以上就是我针对自然语言推理的一个简单介绍，作为自然语言理解的一个重要组成部分，这里边还是有很多很有意思的内容值得研究的。♪(＾∀＾●)ﾉ]]></content>
      <categories>
        <category>notes</category>
      </categories>
      <tags>
        <tag>natural language inference</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo+github搭建个人博客记录]]></title>
    <url>%2F2018%2F05%2F26%2Fblog-start%2F</url>
    <content type="text"><![CDATA[作为一个工科生，但一直希望能够记录一些东西；同样作为一个工科生，有能力自己搭建博客。看了很多大神的github博客，最终决定也用hexo+github搭建一个自己的个人小站，记录下自己的一些足迹。 搭建步骤github建立repository在自己的github上新建repository，要注意的是name必须是username.github.io，该地址也是博客的地址，换成其他名字是不行的 hexo安装安装git和node.js git地址：https://git-scm.com/download/win node.js地址：https://nodejs.org/zh-cn/ 安装Hexo npm install -g hexo #-g表示全局安装, npm默认为当前项目安装 Hexo部署建立博客的根目录，（建议使用全英文目录，避免出现奇奇怪怪的bug），然后在该目录下打开git bash，（这里同样建议使用git bash，不使用windows terminal，因为windows terminal有一些log信息不会输出，这样就看不到哪里有问题了） hexo init #新建博客目录hexo g #根据当前目录下文件生成静态网页hexo s #启动服务器，还可以使用hexo s -p 4321，解决端口占用问题 这样就可以在浏览器中输入localhost:4000查看了， 简单介绍一下文件目录，摘自这里 public：执行hexo generate命令，输出的静态网页内容目录 scaffolds：layout模板文件目录，其中的md文件可以添加编辑 scripts：扩展脚本目录，这里可以自定义一些javascript脚本 source：文章源码目录，该目录下的markdown和html文件均会被hexo处理。该页面对应repo的根目录，404文件、favicon.ico文件，CNAME文件等都应该放这里，该目录下可新建页面目录。 drafts：草稿文章 posts：发布文章themes：主题文件目录 config.yml：全局配置文件，大多数的设置都在这里 package.json：应用程序数据，指明hexo的版本等信息，类似于一般软件中的 关于 按钮 Hexo 写文章 hexo new “postname” #然后在posts目录下的postname.md文件中编辑博客 然后在source目录下打开对应的markdown文件，编辑即可，这里建议使用typora Hexo本地调试 hexo clean #清除之前生成的内容，保证不出问题hexo g #生成hexo s #启动本地服务，进行文章预览调试，也可以使用hexo s -p 4321 Hexo部署到github上首先安装一个插件，保证能够使Hexo部署到GitHub上： npm install hexo-deployer-git –save 在博客目录下找到配置文件_config.yml，进行编辑 编辑前： # Deployment## Docs: http://hexo.io/docs/deployment.htmldeploy: type: 编辑后： deploy: type: git repo: 对应仓库的SSH地址（可以在GitHub对应的仓库中复制） branch: 分支（User Pages为master，Project Pages为gh-pages） 然后执行： hexo ghexo deploy 之后就可以在浏览器中通过username.github.io进行浏览，棒棒哒 优化部署和管理虽然目前已经基本搭建好了博客，但是编辑只能在当前电脑编辑，github上保存的是生成之后的html文件，整个博客的源代码都在本地，那如果想要在别的电脑上编辑怎么办，这时候就要利用到github的分支，即在建立博客仓库的时候，建立两个分支，一个用于展示网站内容，一个用于存放hexo文件， 具体流程参考了这里 创建流程 创建仓库，username.github.io； 创建两个分支：master 与 hexo； 设置hexo为默认分支（因为我们只需要手动管理这个分支上的Hexo网站文件）； 使用git clone git@github.com:username/username.github.io.git拷贝仓库； 在本地username.github.io文件夹下通过Git bash依次执行npm install hexo、hexo init、npm install 和 npm install hexo-deployer-git（此时当前分支应显示为hexo）; 修改_config.yml中的deploy参数，分支应为master； 依次执行git add .、git commit -m “…”、git push origin hexo提交网站相关的文件； 执行hexo generate -d生成网站并部署到GitHub上。 这样一来，在GitHub上的CrazyMilk.github.io仓库就有两个分支，一个hexo分支用来存放网站的原始文件，一个master分支用来存放生成的静态网页。 日常修改在本地对博客进行修改，一般是如下步骤 git pull (在保证本地没有修改的情况下，更新到github上的版本，保持版本一致，非常重要) 进行各种编辑，修改操作 依次执行git add .、git commit -m “…”、git push origin hexo指令将改动推送到GitHub（此时当前分支应为hexo） 然后才执行hexo generate -d发布网站到master分支上 异地修改当在不同的电脑上修改时，一般是如下步骤 使用git clone git@github.com:username/username.github.io.git拷贝仓库（默认分支为hexo） 在本地新拷贝的username.github.io文件夹下通过Git bash依次执行下列指令：npm install hexo、npm install、npm install hexo-deployer-git 然后就是日常修改中的2以后的操作 最后果然自己搭，坑不是一般的多，后续会慢慢更新，记录我在使用过程中踩过的坑↖(^ω^)↗]]></content>
      <categories>
        <category>notes</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F01%2F01%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. test2 Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <tags>
        <tag>others</tag>
      </tags>
  </entry>
</search>
