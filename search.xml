<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge</title>
      <link href="/2019/06/14/2019-05-31/"/>
      <url>/2019/06/14/2019-05-31/</url>
      <content type="html"><![CDATA[<blockquote><p>Title: CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge</p><p>Authors: Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant</p><p>Link: <a href="https://arxiv.org/abs/1811.00937" target="_blank" rel="noopener">https://arxiv.org/abs/1811.00937</a></p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>该文章是NAACL2019 best resource paper.</p><p>在现实环境中，人们回答问题时通常会考虑已拥有的丰富的先验知识（即常识），而当前的大多是QA任务中，并不需要额外的知识，单纯从给定的信息中就可以得出结果。那么这些数据中是否可以学习到常识呢？本文就提出了一种常识问答的数据集，该数据集提供了一个更加复杂的语义环境，需要依靠先验知识（常识）来进行区分。利用该数据集就可以更好的验证模型是否能够从数据中学习到丰富的先验知识，从而为更加复杂的推理等任务提供必要的支撑。</p><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>首先展示该数据集的一个例子，如下图：</p><p><img src="/2019/06/14/2019-05-31/2019-05-31-1.JPG" alt="example"></p><p>在这个例子中，<em>river</em>被称为source concept，其他词被称为target concept。从图中可以看出目标词和源词有着相同的关系AtLocation，但是在问题中，不同的问题需要使用不同尝试才能选择出正确的答案，例如问题1，在晴天把杯子举向空中还能接到水，一般情况下就只能是瀑布了，这里就需要对瀑布有一定的尝试理解。其他问题也是一样的，需要特定的常识信息才能选择出正确答案，单纯根据给定的文本是选择不出正确答案的。</p><p>为了保证所有的问题必须需要常识信息才能推断出正确的结果，作者选择使用conceptNet作为数据来源，然后进行抽取，具体如下：</p><p><img src="/2019/06/14/2019-05-31/2019-05-31-2.JPG" alt="data flow"></p><p>首先是对conceptNet中的一些边进行过滤，例如一些很形式化的关系（有关系），或者可以直接从文本中获取的关系（isA）这种，对词也进行了一些过滤，例如一个concept超过了四个词或者不是英语。同时还考虑了不同词之间的相似关系，编辑距离（edit distance）过小的元组也会被过滤掉。通过这些人工规则保证了初始数据的高质量。</p><p>接下来就是大规模人工标注，首先针对每个source concept，会自动选择三个拥有相同关系的target concept，然后标注人员会被要求针对每个target concept提出一个问题，这个问题必须包含source concept，同时问题的答案只能是一个target concept，不能同时是两个答案。同时为了使问题更加复杂，需要考虑更多的尝试信息，每个标注员还被要求在选项中添加两个额外的答案，一个是来自conceptNet，并且和source concept拥有相同的关系，另一个由标注员自己设计，通过这种设计，保证了问题的复杂性，要求模型必须依靠常识信息进行判断，而不是单纯就本文信息就能得出答案。</p><p>为了保证标注的效果，作者还找了额外的标注员对问题的答案进行判断，每个问题有两个标注员进行判断，至少有一个标注员回答对，这个问题才会被作为有效的数据。以下就是整个数据集的一些关键统计信息</p><p><img src="/2019/06/14/2019-05-31/2019-05-31-3.JPG" alt="key statistics"></p><p>接下来作者展示了要想正确选择答案需要的一些常识，或者说选择出正确答案需要的一个推理过程</p><p><img src="/2019/06/14/2019-05-31/2019-05-31-5.JPG" alt="examples"></p><p>例子中红色的为正确答案，蓝色的为其他concept，可以看出要想回答出正确的答案，模型需要利用充足的常识信息进行推理和判断。而这个过程也可以作为对模型推理机制进行研究的一个方向。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>有了数据集之后，作者选取了多种做语义理解和语义表征的模型，这其中还包括了GPT， BERT之类利用大规模数据进行预训练，并在多个NLP任务上取得很好的效果的模型，相关结果如下：</p><p><img src="/2019/06/14/2019-05-31/2019-05-31-7.JPG" alt="results"></p><p>为了更好的验证模型的效果，作者还对数据集进行了划分，一种是随机划分，另一种是根据source concept进行的划分，训练集和测试集之间的source concept之间是没有重合的，从结果上看，随机划分的难度更大，而且所有的模型表现效果都远远低于人类的水平，即使是使用了大规模数据进行预训练的BERT-large，这也表明了BERT这种利用语言模型和大规模文本进行预训练的模型更多的是学习到了特定的语义模式，并没有从这些文本中学习到抽象的知识。这也可以认为是未来BERT的一个改进方向。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>现在已经有越来越多的研究开始关注于可解释性，抽象语义表达，知识表示，推理等高层次的语义理解，而不是单纯的利用大规模文本来学习特定的语义模式，自然语言处理也开始转向了理解，学习等人类特有的能力，通过不同的数据和特别设计的任务去验证模型在这方面的能力，从而推动整个自然语言处理领域向自然语言理解领域进发。这其中还有非常多有意思的内容值得我们去发现，去研究，去解决。</p>]]></content>
      
      <categories>
          
          <category> paper_reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> dataset </tag>
            
            <tag> question answering </tag>
            
            <tag> commonsense </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>From Recognition to Cognition: Visual Commonsense Reasoning</title>
      <link href="/2019/06/14/2019-05-30/"/>
      <url>/2019/06/14/2019-05-30/</url>
      <content type="html"><![CDATA[<blockquote><p>Title: From Recognition to Cognition: Visual Commonsense Reasoning</p><p>Authors: Rowan Zellers, Yonatan Bisk, Ali Farhadi, Yejin Choi </p><p>Link: <a href="https://arxiv.org/abs/1811.10830" target="_blank" rel="noopener">https://arxiv.org/abs/1811.10830</a></p></blockquote><h2 id="Movitation"><a href="#Movitation" class="headerlink" title="Movitation"></a>Movitation</h2><p>当前Visual-to-Language (V2L)已经变成一个非常热门的领域，通过建模文本与图像，人们可以赋予机器更多的智能。例如图像描述生成（Image Captioning），视觉问答（Visual Question Answering）等。但机器是否真的拥有了智能，能够像人一样进行识别推理，还是仅仅抽取了相应的特征，通过发现视觉和语言中的一些特定模式来完成任务呢？很多研究人员更倾向于后者。如下图：当给人一张图时，人可以轻松的推断出画面中的人的行为，目的以及状态，例如我们可以很自然从图中看出people 4告诉people 3煎饼是people 1定的，并且给出恰当的理由，但这对机器来说就很困难的，因此为了更好的进行推理方面的研究，作者提出了Visual Commonsense Reasoning（VCR）任务，并公布了一个同名的数据集。</p><p><img src="/2019/06/14/2019-05-30/2019-05-30-1.JPG" alt="example"></p><h2 id="Task-amp-Dataset"><a href="#Task-amp-Dataset" class="headerlink" title="Task &amp; Dataset"></a>Task &amp; Dataset</h2><h3 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h3><p>和传统的VQA之类的任务不同地方在于，该任务不仅仅会提供图片和问题，同时作者利用Fast-RCNN等工具对图片中的目标进行了识别，并且使用了特定的目标标签进行标记，例如上图中的人全都被标记为person 1， person 2，person 3等，这样模型可以迅速找到目标，从而专注于对图像和文本的理解和推理；第二个不同的地方在于，问答并不是传统的有几个人，什么颜色的衣服等这些很简单很直观的问题，而是使用自然语言句子进行解释推理，更多是问为什么，接下来将会做什么等问题，从下图也可以看出，该数据集中的问题更多的是解释，活动描述等；第三个不同的地方在于，模型不仅仅需要选择出正确的答案，更要选择出为什么这么选的理由。这点也是我认为最巧妙的地方，目前也有一些任务需要对结果进行解释，但要不是进行可视化，要不是生成自然语言的解释，这么做的效果就会很差，因为自然语言生成本来就是一个很有挑战的问题。而在该任务中只专注于对图像文本的理解，答案选择出来的，解释也是选择出来的，因此模型能够更专注于解释和推理，而且直接用准确率进行效果的验证，也非常的简单直观有效。</p><p><img src="/2019/06/14/2019-05-30/2019-05-30-2.JPG" alt="categories"></p><p>考虑到该任务中有问题，答案，解释三个，因此该任务还有两个子任务：1）问题-&gt;答案：只是根据图像和问题进行答案的选择，推理难度稍低；2）问题+答案-&gt;解释：更进一步，根据图像，问题和答案，选择出合理的解释，相对前一个子问题稍难。通过这种多角度的分析验证，从而更好的对模型效果的评估。</p><h3 id="Data-Collection"><a href="#Data-Collection" class="headerlink" title="Data Collection"></a>Data Collection</h3><p>该数据集的图像来自于电影片段，因此大部分的画面都拥有复杂的情境而且情况也各不相同。为了保证抽取到的画面都比较复杂，作者提出了一种“interestingness filter”来确保图像的质量，大致的要求有：首先图片中至少有两个人，其次作者手动标了2000张图片，然后利用这个标记后的数据集训练一个分类器，用以区分图像是否有趣，在大规模人工标注阶段，还会让标注员回答这张图是非常有趣，一般般，还是很无聊，然后利用这些数据再次训练一个更大的分类器，再次进行数据的过滤，最后选择出最有意思的40k的图片。</p><p>关于问题和答案，在大规模人工标注阶段，标注员被要求问一个问题，然后回答他，并且提供相应的解释，为了保证文本的质量，作者同时加入了自动质量检测机制，每个问题必须包含4个词，答案必须包含3个词，解释必须包含5个词，并且这些文本中至少包含一个目标。当然后续作者还进行了质量检测等多个程序，从而保证整个数据集的质量。具体可以参考原文的索引，下图展示了数据集的一些统计信息</p><p><img src="/2019/06/14/2019-05-30/2019-05-30-6.JPG" alt="result 1"></p><p><img src="/2019/06/14/2019-05-30/2019-05-30-7.JPG" alt="result 2"></p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>介绍完任务和数据，作者在本文中也提出了一种baseline方法，具体结构图如下：</p><p><img src="/2019/06/14/2019-05-30/2019-05-30-4.JPG" alt="model structure"></p><p>该模型分为三部分：Grounding，Contextualization，Reasoning，</p><h3 id="Grounding"><a href="#Grounding" class="headerlink" title="Grounding"></a>Grounding</h3><p>该部分主要是是学习图像文本的联合表达，因为问题和答案都包含了图像中的目标标签以及自然语言描述，因此可以使用图像信息对文本信息进行增强表达，作者使用Bi-LSTM来处理所有的文本，因为目标标签可以和图像上的具体区域进行对应，因此只有那些目标标签部分会同时加入对应的图像信息，这些图像信息使用上了CNN来学习特征表达，然后就得到了所有文本的增强表达。</p><h3 id="Contextualization"><a href="#Contextualization" class="headerlink" title="Contextualization"></a>Contextualization</h3><p>该部分主要是为了建模问题和答案之间的交互，这里就是用了注意力机制，作者在文中使用了co-attention来处理，最后就得到了问题的另一种表示，另外，为了使用图像对答案进行增强表示，防止有些信息在grounding阶段没有被获取到，作者使用了另一个注意力机制对答案信息进行增强。</p><h3 id="Reasoning"><a href="#Reasoning" class="headerlink" title="Reasoning"></a>Reasoning</h3><p>最后一个阶段，作者使用将问题，答案，以及图像中的目标送给一个Bi-LSTM进行处理，然后在所有的隐层状态上进行最大池化(max-pooling)，最后再使用一个MLP进行理由的选择。</p><p>整个模型还是比较简单的，为了获得更好地效果，作者还使用了BERT对文本进行处理，相关的实验结果如下：</p><p><img src="/2019/06/14/2019-05-30/2019-05-30-8.JPG" alt="result-4"></p><p>作者选择了一些只用文本的模型，一些使用VQA的模型，因为这是一个更倾向于推理的任务，因此可以看到作者提出的模型效果还是最好的，但和人类相比就差的有点多了，这也说明了这个问题还远远没有解决，还有很大的研究空间。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>V2L是当前非常有一个的一个研究方向， 和纯CV，NLP不同的地方在于，它需要同时考虑图像文本的信息，相当于既有直观的视觉信息，也有抽象的语义信息，如何相互融合，如何相互协助就成了一个非常值得研究的的问题。当前很多V2L的任务其实并没有很好的去验证这两部分要求，通常都会倾向于其中的一个方面，而VCR这个任务则巧妙的提出了需要根据图像来推理接下来可能的行为，或者解释图中的行为，通过这种要求迫使模型更加专注于对文本图像的协同理解，而且对应的数据集最后落脚为一个分类任务，从而保证了模型能更专注于语义的理解和推理。这是一个非常有意思的问题，值得仔细研究一番。</p>]]></content>
      
      <categories>
          
          <category> paper_reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> visual-to-language </tag>
            
            <tag> visual reasoning </tag>
            
            <tag> VCR </tag>
            
            <tag> dataset </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ERNIE: Enhanced Language Representation with Informative Entities</title>
      <link href="/2019/06/14/2019-05-29/"/>
      <url>/2019/06/14/2019-05-29/</url>
      <content type="html"><![CDATA[<blockquote><p>Title: ERNIE: Enhanced Language Representation with Informative Entities </p><p>Authors: Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu</p><p>Link: <a href="https://arxiv.org/abs/1905.07129" target="_blank" rel="noopener">https://arxiv.org/abs/1905.07129</a></p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>自从BERT被提出之后，整个自然语言处理领域进入了一个全新的阶段，大家纷纷使用BERT作为模型的初始化，或者说在BERT上进行微调。BERT的优势就在于使用了超大规模的文本语料，从而使得模型能够掌握丰富的语义模式。但BERT是否还有改进的空间呢？如下图，当提到<em>鲍勃迪伦</em>的时候，单纯依靠大规模的普通文本很难理解到底是指音乐家还是作者，但如果加入充分的先验知识，那么模型可能就会学习到更加精细化的语义表示，如何让BERT掌握更多的人类先验知识呢？本文就提出了一种方法，将知识图谱的信息加入到模型的训练中，这样模型就可以从大规模的文本语料和先验知识丰富的知识图谱中学习到字，词，句以及知识表示等内容，从而有助于其解决更加复杂，更加抽象的自然语言处理问题。</p><p><img src="/2019/06/14/2019-05-29/2019-05-29-1.JPG" alt="example"></p><h2 id="Model-Structure"><a href="#Model-Structure" class="headerlink" title="Model Structure"></a>Model Structure</h2><p>整个模型的动机就是将知识图谱的信息有效融入到模型的训练中，考虑到BERT的复杂结构，如何将知识图谱的信息进行有效融合呢？作者提出了如下的结构：</p><p><img src="/2019/06/14/2019-05-29/2019-05-29-2.JPG" alt="model structure"></p><p>可以看出，模型将BERT中的Encoder替换为了T-Encoder+K-Encoder，T-Encoder依然是对原来的文本进行编码，这部分和BERT是一样的，在K-Encoder中，可以看到输入输出都变成了两个，多了entity的信息。具体来说，首先可以利用TransE的方法对知识图谱中的内容进行表示，并对文本中的实体进行识别，这样文本中的实体都会有一个来自知识图谱的实体表示，需要注意的是文本的长度和实体的长度并不相等，然后先用mutli-head attention对文本和实体分别进行处理，得到在整个序列中情境感知的语义表示：</p><p><img src="/2019/06/14/2019-05-29/2019-05-29-4.JPG" alt="mh-att"></p><p>接下来就是对这两种信息进行融合，或者说利用实体的信息来增强对文本语义的理解，这个时候就分成两种情况：</p><ol><li>文本中的词有实体对应，一个很简单的思路，通过一个非线性变换，得到融合后的信息：</li></ol><p><img src="/2019/06/14/2019-05-29/2019-05-29-5.JPG" alt="aligned entity"></p><ol start="2"><li>文中的词没有实体对应，为了保证一致性，还是同样的方法，只是只有实体词的输入：</li></ol><p><img src="/2019/06/14/2019-05-29/2019-05-29-6.JPG" alt="no aligned"></p><p>通过这样的方法，就将实体的知识信息融入到了对文本语义的增强表示中，接下来将相应的单元重复多次，就得到的最终的文本语义表示。</p><p>从模型上实现了知识图谱信息的有效融合，那如何训练么？如果单纯还是和BERT的训练方式相同，知识图谱的知识信息可能并不能如期望的那样进行有效融合，因此作者参考Masked Language Model设计了一个denoising Entity Auto-encoder (dEA)任务，用以训练模型对实体信息的感知和对齐，具体内容如下</p><h2 id="Training-details"><a href="#Training-details" class="headerlink" title="Training details"></a>Training details</h2><p>dEA的目的就是要求模型能够根据给定的实体序列和文本序列来预测对应的实体，首先是实体和文本之间的对齐概率计算：</p><p><img src="/2019/06/14/2019-05-29/2019-05-29-7.JPG" alt="aligned entity distribution"></p><p>这个公式也被当作训练dEA时的损失函数，有了目标，那么数据该如何准备呢？和Masked Language Model类似，作者对实体也做了如下处理：</p><ol><li>对于一个给定的文本-实体对应序列，5%的情况下，实体会被替换为一个随机的实体，这么做是为了让模型能够区分出正确的实体对应和错误的实体对应；</li><li>对于一个给定的文本-实体对应序列，15%的情况下，实体会被mask，这是为了保证模型能够在文本-实体没有被完全抽取的情况下找到未被抽取的对应关系；</li><li>对于一个给定的文本-实体对应序列，剩下的80%的情况下，保持不变，这是为了保证模型能够充分利用实体信息来增强对文本语义的表达。</li></ol><p>和BERT类似，作者也对输入进行了一些调整，从而保证了模型能够自适应不同的任务，下图展示了针对三类自然语言处理任务的输入调整：</p><p><img src="/2019/06/14/2019-05-29/2019-05-29-3.JPG" alt="dEA"></p><p>对于一般的NLP任务而言，知识在输入的头尾加上特定的开始结束符号，然后送给模型，并取[CLS]对应的输出作为输入句子的表示。对于一些知识驱动的任务，如关系分类，实体类别识别等任务，作者加入了特定的符号用以区分这些任务，对于实体类别识别任务，作者加入了 [ENT]来指导模型使用文本表示和实体表示进行最后的实体信息识别；对于关系分类任务，作者加入了[HD]和[TL]分别表示头实体和尾实体，然后使用[CLS]对应的特征向量来进行最后的分类。整个这部分的操作和GPT-2的无监督学习有些类似。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>和BERT不同的是，作者首先在两个知识驱动的任务上进行了模型效果的验证：</p><ol><li>Entity Typing：给定实体和对应的上下文，模型需要识别该实体的语义类型</li><li>Relation Classification：给定一个句子，模型需要识别出句子中的两个实体之间的关系</li></ol><p>这两个任务都是知识驱动的任务，不单单需要模型能够掌握丰富的语义模型，同时需要模型能够有丰富的先验知识，这样才能进行准确识别和分类，下图是对应的实验结果：</p><p><img src="/2019/06/14/2019-05-29/2019-05-29-8.JPG" alt="result 1"></p><p><img src="/2019/06/14/2019-05-29/2019-05-29-9.JPG" alt="result 2"></p><p>从实验结果上看，模型取得了非常好的效果，这也说明了模型有效融入了知识图谱的先验信息，实现了文本语义的增强表示，同时作者还在常见的NLP任务上和BERT base进行了对比，实验结果也证实了模型的有效性</p><p><img src="/2019/06/14/2019-05-29/2019-05-29-10.JPG" alt="result 3"></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>自从BERT被提出后，NLP领域的研究形式也开始慢慢向CV靠拢，利用这些预训练好的超大模型获得丰富的语义模式，从而通过在具体任务上的微调获得最后的效果。但NLP和CV还是有很大不同的，自然语言是一种高度抽象的信息，单纯通过语言模型获取丰富的语义模式并不能一劳永逸的解决所有问题，而这篇文章就进行了这方面的一个尝试，通过引入外部先验知识增强模型的语义理解和表征。那是是不是还可以进行不仅仅是文本方面的增强，例如通过语音融入情感信息，通过图像引入视觉信息等，这些都是值得研究的地方。</p>]]></content>
      
      <categories>
          
          <category> paper_reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BERT </tag>
            
            <tag> knowledge base </tag>
            
            <tag> sentence </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions</title>
      <link href="/2019/04/03/2019-04-02/"/>
      <url>/2019/04/03/2019-04-02/</url>
      <content type="html"><![CDATA[<blockquote><p>Title: Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions </p><p>Authors: Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara </p><p>Link: <a href="https://arxiv.org/abs/1811.10652" target="_blank" rel="noopener">https://arxiv.org/abs/1811.10652</a></p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Image captioning作为Visual-to-Language中一个十分传统的研究领域，每年都会提出一些新的方法，新的模型来生成符合图像的描述。但是整个过程到目前为止一直都是end-to-end，或者说是一个黑盒模型。而同一个图像可以有无数种不同的表述方法，注意力机制虽然可以帮助模型聚焦到图像的某一块，但该过程基本上是不可控的。如何增加生成过程的可控性，根据需要生成相应的描述称为image captioning的一个新的研究方向。而且该方向明显具有更大的应用潜力和实用价值。如下图：利用图像特征，甚至加上注意力机制，但最终生成的描述都是比较类似的。当对需要生成的对象进行控制的时候，就可以生成各有侧重性的描述，而且通过对应关系，也使得结果更合理可信。</p><p><img src="/2019/04/03/2019-04-02/2019-04-02-1.JPG" alt="example"></p><p>因此本文就提出了一种新的image captioning的结构，通过对图像的region进行控制，从而生成更加多样化的描述。整体的出发点还是十分有意思的。</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>在介绍完整的模型之前，先简单介绍一个前提：自然语言天然是层次化的结构。这点很好理解，在传统的自然语言句子中，有细粒度的单词，然后是短语，然后是句子，接着是段落，文章等。本文提出了可以利用文中的名词将整个句子分成多个名词块（noun chunks），每个名词块由核心名词和对应的描述词组成。而这些名词块又能和图像的不同区域对应起来。这样，多样化的句子就可以通过对这些块的不同考虑，也就是通过考虑图像中的不同区域，从而生成不同的句子描述。下图给了这样的一个例子:</p><p><img src="/2019/04/03/2019-04-02/2019-04-02-2.JPG" alt="example 2"></p><p>那么接下来就是模型的整体框架结构：</p><p><img src="/2019/04/03/2019-04-02/2019-04-02-3.JPG" alt="model structure"></p><p>和传统的image captioning模型不同的地方在于：该模型的输入是图像$I$和一个region序列$R = (r_0, r_1, …, r_N)$，后者就作为控制信号控制整个生成过程。</p><h3 id="Region序列"><a href="#Region序列" class="headerlink" title="Region序列"></a>Region序列</h3><p>作为整个模型中非常核心的一部分，region序列关系到整个模型的生成质量，那么首先需要解决两个问题，1）region如何得到，2）region的序列如何控制。第一个很好得到，直接使用Faster-RCNN对图像进行目标检测就可以了，就可以得到整个图像中的目标特征表示，针对第二个问题，本文使用了一个sorting network来解决这个问题，具体如下：</p><p><img src="/2019/04/03/2019-04-02/2019-04-02-5.JPG" alt="Sinkhorn Network"></p><p>首先输入包括：1）Faster-RCNN输出的目标特征表示（2048维）；2）使用Glove对每个region的类别进行embedding，因为每个类别可以用单词表示；3）归一化之后的目标框的位置和大小（4维）。接下来利用上图中表示的全连阶层进行处理，最后将其映射到一个N-dimension的描述向量，如果同一个region有多个检测目标，那么将所有的表示向量做average-pooing，保证每个region有一个向量表示，这里需要说明的一点是：最后得到的向量的维度适合检测到的region数是一样的，也就是说会有N个region，这是因为接下来的Sinkhorn network处理的是一个仿真，这点需要注意。</p><p>正如上文所述：当对所有的region进行处理之后，就得到了一个$N * N$的矩阵，接下来就可以通过Sinkhorn操作将其转化为一个 “soft”排列矩阵 $P$，具体就是执行L次连续的行归一化和列归一化，</p><p><img src="/2019/04/03/2019-04-02/2019-04-02-6.JPG" alt="formulation 1"></p><p>当经过L次处理之后，通过Hungarian算法将结果转换为排列矩阵，就得到了排好序的region表示，</p><h3 id="Model-Details"><a href="#Model-Details" class="headerlink" title="Model Details"></a>Model Details</h3><p>解决了输入的问题，接下来就是模型的主体部分，因为模型加入了一个控制信号，因此再生成的时候，不仅需要考虑生成 句子的合理性，$p(y_t|R,I)$，同时需要考虑生成的句子是符合给定的region序列的，也即句子中的名词块的转换概率是和输入的region序列是一致的，$p(g_t|R,I)$，其中$g_t$是一个布尔类型的转换门。</p><p>有了目标，接下来就是整个模型的生成过程了，因为要生成一个句子，因此RNN是一个非常好的框架，在生成每个隐层状态时，需要考虑前一个状态，当前的词，当前的region表示，即$h_t = RNN(\omega_t, r_t, h_{t-1})$，其中region的选择是通过上文提到的region转换门$g_t$实现的，具体如下：</p><p><img src="/2019/04/03/2019-04-02/2019-04-02-7.JPG" alt="formulation 2"></p><p>那么接下来就是这个转换门$g_t$的计算方法了</p><p>因为整个模型中都是使用LSTM作为base model的，因此，在计算转换门的值的时候也需要考虑LSTM的状态相关信息，本文首先计算出一个哨兵值：</p><p><img src="/2019/04/03/2019-04-02/2019-04-02-8.JPG" alt="formulation 3"></p><p>其中$h_{t-1}$是前一个时刻的隐状态值，$m_t$是当前时刻的cell memory值，这个计算方法还是很有意思的，一般使用LSTM的时候只会使用隐状态或者cell memory值，这里直接使用了两个，在得到哨兵值之后，本文接着计算隐状态和哨兵值之间的相似度，以及隐状态和当前region中每个表示之间的相似度（从这里可以看出，每个region其实最后是使用了一个矩阵表示的，这点目前存疑）</p><p><img src="/2019/04/03/2019-04-02/2019-04-02-9.JPG" alt="formulation 4"></p><p>接下来就可以计算转换region的概率了，即</p><p><img src="/2019/04/03/2019-04-02/2019-04-02-10.JPG" alt="formulation 5"></p><p>计算出转换门$g_t$的值之后，也就意味着建立了控制信号$R​$和生成过程之间的联系，接下来本文继续利用上一阶段的哨兵机制进行描述的生成。首先还是根据视觉信息生成一个哨兵向量，该向量控制模型是否要考虑当前region：</p><p><img src="/2019/04/03/2019-04-02/2019-04-02-11.JPG" alt="formulation 7"></p><p>接下来利用additive attention计算出在当前region和哨兵向量上的注意力分布：</p><p><img src="/2019/04/03/2019-04-02/2019-04-02-12.JPG" alt="formulation 8"></p><p>常规操作，通过这样的方式，就可以得到一个加权向量，从而作为LSTM在当前时刻的输入：</p><p><img src="/2019/04/03/2019-04-02/2019-04-02-13.JPG" alt="formulation 9"></p><p>正如前文提到的，模型需要考虑两个目标，生成 句子的合理性，$p(y_t|R,I)$，以及句子中的名词块的转换概率是和输入的region序列是一致的，$p(g_t|R,I)$，因此目标函数也需要考虑这两部分：</p><p><img src="/2019/04/03/2019-04-02/2019-04-02-14.JPG" alt="formulation 9"></p><p>其中$y_{1:t-1}^<em>$和$r_{1:t}^</em>$表示ground truth，两部分的损失均用交叉熵计算得到。同时为了更好的训练模型，本文在使用交叉熵训练过之后，同时使用了强化学习进行调优。具体细节不在这里赘述了，感兴趣的可以看一下原文。</p><p>以上就是整个模型的技术细节</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>本文使用Flickr30K entities和COCO entities两个数据集进行模型的训练和测试，同时根据region的不同设定（i.e.，a sequence of regions, a set of regions）对实验结果进行了评价，下文选取部分测试结果进行展示：</p><p><img src="/2019/04/03/2019-04-02/2019-04-02-15.JPG" alt="result 1"></p><p><img src="/2019/04/03/2019-04-02/2019-04-02-16.JPG" alt="result 2"></p><p>从实验结果上模型几乎在所有评价结果上都优于比较的baseline模型，充分说明了模型的有效性，同时本文也展示了一些实验结果，从而说明本文提出了控制方式的有效性</p><p><img src="/2019/04/03/2019-04-02/2019-04-02-4.JPG" alt="case study"></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>针对大多数image captioning模型的黑盒结构，本文提出了一种通过外部控制信号显式的控制整个文本的生成，根据region及其顺序的不同而生成不同描述重点的句子。首先，因为对图像中的信息更加明确，所以生成的结果更贴近图像，效果更好，其次通过对描述重点的显式考虑，使得最后的生成句子多样性更好。从这两个角度实现了描述生成质量的提升。文章的想法还是非常有意思的，可能对image caption这部分内容了解的不是很多，文中有些地方还是有些疑惑的，感觉作者并没有讲清楚。不过并不影响这是一篇非常值得读的文章。</p>]]></content>
      
      <categories>
          
          <category> paper_reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Image </tag>
            
            <tag> Sentence </tag>
            
            <tag> Image Caption </tag>
            
            <tag> Sentinel </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog</title>
      <link href="/2019/04/01/2019-03-29/"/>
      <url>/2019/04/01/2019-03-29/</url>
      <content type="html"><![CDATA[<blockquote><p>Title: Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog </p><p>Authors: Zhe Gan, Yu Cheng, Ahmed EI Kholy, Linjie Li, Jingjing Liu, Jianfeng Gao</p><p>Link: <a href="https://arxiv.org/abs/1902.00579" target="_blank" rel="noopener">https://arxiv.org/abs/1902.00579</a></p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>当前Visual dialog模型尽管已经取得了令人印象深刻的表现，但当问题涉及到图像的特定区域或者比较细节的对话片段时，模型就很难给出准确的答案。究其原因，单步的分析需要同时关于太多的信息，当需要特定的位置或者说需要对问题和图像，对话历史进行反复理解时，单步理解就收到了很大的限制。因此多步推理，从粗粒度到细粒度就显得十分必要的，这样模型才能关注到具体的细节，从而给出准确的答案，例如下图：</p><p><img src="/2019/04/01/2019-03-29/2019-03-29-1.JPG" alt="example"></p><p>单步分析时，需要知道<em>He</em>指代的是历史对话中的<em>young boy</em>，还需要在图像信息中关注到<em>young boy</em>，然后才能具体知道他是否穿了短裤，但如果使用多步的话，第一步会从历史对话和图像中选出来和问题有关的部分，舍弃到并没有太大关联的地方，第二步就可以在这些相关的地方进行精细化的分析，从而准确得到答案。基于这样的一个想法，本文提出了一种多步的双重注意力模型，用于visual dialog。</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>还是先上整个模型框架图：</p><p><img src="/2019/04/01/2019-03-29/2019-03-29-2.JPG" alt="model structure"></p><p>模型框架图加个外框感觉就好看很多，可以学习学习。从框架中可以看出模型主要包含三个模块：1）memory generation：生成一套文本和图像的memory，用于下一步的reasoning；2）Multi-step Reasoning：使用循环双重注意力去联合编码问题，图像和对话历史，用于接下来的解码；3）Answer Decoding：使用上一步得到的联合编码向量去推断答案。</p><h3 id="Memory-Generation-Module"><a href="#Memory-Generation-Module" class="headerlink" title="Memory Generation Module"></a>Memory Generation Module</h3><p>首先是对数据的处理，对图像，使用Faster-RCNN抽取图像特征，$F_I = R-CNN(I) \in \mathbb{R}^{n_f * M}$，$I$表示的图像信息，这样就得到了图像中的object的向量表示，在这里$n_f = 36, M=2048$，也就是抽取了36个图像特征，同时使用一个非线性变换得到将这些特征映射到和问题相同的特征空间去 $M_v = tanh(W_IF_I)$.</p><p>对于文本的话，本文是直接将图像描述和历史对话完全拼接起来，这样就得到了一个长句子$H=(u_1, u_2, …, u_L)$，$u_i$表示第i个词，L表示最大的句子长度，然后使用预训练的embedding进行编码，双向LSTM进行处理，最终就得到了文本的特征表示$M_d = [h_1, h_2, …, h_L]$，具体实现如下：</p><p><img src="/2019/04/01/2019-03-29/2019-03-29-6.JPG" alt="formulation 1"></p><h3 id="Multi-step-Reasoning-Module"><a href="#Multi-step-Reasoning-Module" class="headerlink" title="Multi-step Reasoning Module"></a>Multi-step Reasoning Module</h3><p>该模块的整体框架是一个RNN，每一步的隐状态表示的问题的当前表示，并会作为query去检索文本和图像信息，具体流程可以表示为$s_t \rightarrow v_t \rightarrow d_t \rightarrow s_{t+1}$，这个可以看作one-step reasoning，通过这样的一个循环方式，从而实现对细节的把握，从而最终给出正确的答案。</p><p>首先还是会利用一个双向LSTM对问题进行编码，得到问题的隐状态表示$M_q = [q_1, q_2, …, q_K]$，接下来是首先利用self-attention对问题进行处理，从而得到问题的语义表示，而这些权重表明了问题中哪些词是非常重要的，并且过滤了那些不重要的词</p><p><img src="/2019/04/01/2019-03-29/2019-03-29-7.JPG" alt="formulation 2"></p><p>同时该状态会作为multi-step reasoning rnn的隐层初始状态（因为RNN的隐层状态都是对问题的语义表示），接下来就是不断使用注意力机制关注图像和历史对话中的重要信息，并用这些重要信息更新对问题的表示，从而一步步推进，最终得到正确的答案</p><ol><li>$(s_t, d_{t-1}) \rightarrow v_t$</li></ol><p><img src="/2019/04/01/2019-03-29/2019-03-29-8.JPG" alt="formulation 3"></p><p>该步骤利用问题的隐状态表示和上一步的历史信息表示，得到图像上那些地方需要被关注，从而更新图像的需要关注表示，这里的公式应该是写错了，第一个公式的$d_t$应该是$d_{t-1}$的</p><ol start="2"><li>$(s_t, v_t) \rightarrow d_t$</li></ol><p><img src="/2019/04/01/2019-03-29/2019-03-29-9.JPG" alt="formulation 4"></p><p>相同的操作，利用一直的信息更新历史对话中需要被关注的地方，得到历史对话的新的表示</p><ol start="3"><li>$(v_t, d_t) \rightarrow s_{t+1}$</li></ol><p>到这一步，已经得到了更新之后的图像特征表示$v_t$和历史对话特征表示$d_t$，一般情况下可以简单使用相同的注意力计算方法直接得到问题的更新表示，但这一步的意图和之前的是不同的，在之前的两步，是为了获取更多的有用的信息，因此使用注意力机制不断更新需要关注的地方，而这一步应该是将这些选择出来的信息进行融合，为回答问题提供辅助，也即这一步需要的是通过融合，得到送给RNN的输入，通过RNN的隐层状态更新，从而得到问题的新的表示。因此这里使用了MFB的方式来进行更新</p><p><img src="/2019/04/01/2019-03-29/2019-03-29-10.JPG" alt="formulation 5"></p><p>$z_t$表示的是t时刻RNN的输入，得到输入之后，利用RNN进行更新，这里使用的是GRU，即$s_{t+1} = GRU(s_t, z_t)$，</p><p>经过T轮之后，就得到了一系列的${s_t, v_t, d_t}_{t=1}^T$，本文对这些内容再次进行融合，从而得到最后的语义表示向量</p><p>$$c_t = [MFB(s_t, v_t), MFB(s_t, d_t), MFB(v_t, d_t)], \quad C= {c_t}_{t=1}^T$$</p><h3 id="Answer-Decoding-Module"><a href="#Answer-Decoding-Module" class="headerlink" title="Answer Decoding Module"></a>Answer Decoding Module</h3><p>本文首先还是利用双向LSTM和self-attention对问题进行编码，得到每个问题的表示$a_j$，然后在问题和语义表示向量之间做点乘，得到一个相似度矩阵S, $S[t, j] = c_ta_j^T$。</p><p>在此基础上，需要使用不同的策略获取最后的答案，在这里作者使用了三种不同的策略：1）直接使用最后一个表示进行预测，$o = S[T, :]$； 2）使用所有步的平均表示 $o = \frac{1}{T}\sum_{t=1}^TS[t, :]$；3）使用随机dropout：在训练的时候以一定概率随机丢弃某些步的结果，用剩下的结果的平均进行组后预测，在测试的时候使用所有步的平均进行预测（这个不能算是单独的一个步骤吧，感觉就是训练时加了dropout）</p><p>在训练时，本文考虑了两种损失函数，交叉熵和n-pair损失，交叉熵好理解，将$o$送个一个分类层，n-pair损失的计算方式如下：假设第g个是正确答案，则损失定义为</p><p><img src="/2019/04/01/2019-03-29/2019-03-29-11.JPG" alt="loss function"></p><p>以上就是整个模型的技术细节</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>本文使用的数据集是<a href="https://visualdialog.org/" target="_blank" rel="noopener">visual dialog V1.0</a>，评价指标是(Normalized Discounted Cumulative Gain, NDGC)，相关实验结果如下：</p><p><img src="/2019/04/01/2019-03-29/2019-03-29-3.JPG" alt="experiment results"></p><p>可以看出模型的实验结果是明显好于baseline的，而且随着reasoning的步数增加，模型的效果是在不断变好的，但这部分感觉可以测试更多的步骤，看看是否效果有明显的提升</p><p>接下来是一些case study</p><p><img src="/2019/04/01/2019-03-29/2019-03-29-4.JPG" alt="case study"></p><p>可以看出来模型是逐步从粗粒度到细粒度，慢慢关注到具体的细节，从而最终给出正确的答案，这部分感觉还是十分有意思的，而且在问题的生成模型中，可以看出模型不仅可以给出正确的答案，同时给出具体的细节内容，使回答更丰富，更具有说服力，如下图：</p><p><img src="/2019/04/01/2019-03-29/2019-03-29-5.JPG" alt="case study on generation"></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本文针对当前模型无法回答细节问题，设计了一个multi-step reasoning的方法，从粗粒度到细粒度不断去更新需要关注的信息，从而使得模型能够根据已掌握的信息不断更新需要关注的内容，从而最终取得了正确的答案。这个思路感觉和人的一些习惯非常类似，我们并不是一步就得出了答案，而是根据已有知识不断更新自己的关注点以及对问题的理解，从而最终得到正确的答案，<em>罗马不是一天建成的</em>，这个思路非常值得关注。</p>]]></content>
      
      <categories>
          
          <category> paper_reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Image </tag>
            
            <tag> Visual Dialog </tag>
            
            <tag> VQA </tag>
            
            <tag> Sentence </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Image-Question-Answer Synergistic Network for Visual Dialog</title>
      <link href="/2019/04/01/2019-03-28/"/>
      <url>/2019/04/01/2019-03-28/</url>
      <content type="html"><![CDATA[<blockquote><p>Title: Image-Question-Answer Synergistic Network for Visual Dialog</p><p>Authors: Dalu Guo, Chang Xu, Dacheng Tao </p><p>Link: <a href="https://arxiv.org/abs/1902.09774" target="_blank" rel="noopener">https://arxiv.org/abs/1902.09774</a></p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p><img src="/2019/04/01/2019-03-28/2019-03-28-1.JPG" alt="example"></p><p>Visual Dialog作为Visual-to-Language中十分重要的一个应用，受到了越来越多的关注。如上图所示，该任务主要实施根据给出的图像，图像描述，历史对话，问题挑选出或者生成对应的答案，可以认为是一种更加复杂的VQA。典型的Visual Dialog模型一般都是融合图像，问题和历史对话内容，然后搜索或者生成最适合的答案，这就出现了一个问题，模型会更倾向于输出安全的答案，例如：yes，no等，这些答案符合情境。但是针对对话而言，这类非常短的答案完全就是对话终结者，让人找不到继续对话下去的理由。因此，在Visual Dialog中也需要考虑最后的答案扮演的角色，而不仅仅是将其作为一个输出。因此本文提出了一种图像-问题-答案协同模型，将传统的一阶段模型扩展为两阶段模型，在第一阶段根据答案和图像，问题的关联性进行粗略的选择或评分，在第二阶段，那些概率高的答案会通过与图像和问题的协同分析，以一个更细的粒度进行重新排序，从而最终选出更加符合情境的答案。</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>首先是模型框架：</p><p><img src="/2019/04/01/2019-03-28/2019-03-28-3.JPG" alt="model structure"></p><p>模型主要分为两阶段：第一阶段：对结果进行粗略的打分，第二阶段：对概率高的结果进行细粒度的分析，从而更好的选择出符合情境的答案。</p><p>首先是对模型的所有输入的处理，对图像，本文使用了Faster-RCNN来得到图像的特征表示$V = (v_1, v_2, …, v_n)$，对于问题$q_t$，使用一个LSTM处理，得到问题的表示$m_t^q = LSTM(q_t)$，对于历史对话（包含图像描述）， $H = (H_0, … , H_{t-1})$，其中$H_0$表示的是图像描述，而$H_i$是第i轮的问题和答案的拼接，$H_i=(q_i, a_{i,gt})$，同样也是使用LSTM处理历史对话，得到历史特征$U=(u_0, u_1, …, u_{t-1})$，其中$u_i = LSTM(H_i)$，是LSTM的最后一个隐层状态。这样就得到了标准的输入，从而可以开始两阶段的建模分析。</p><p>在介绍两阶段的细节之前，首先介绍本文中使用的比较多的一个注意力计算方法，multi-modal factorized bilinear pooling (MFB)，这种方法可以可以克服两种特征分布的不同（例如：编码问题和历史对话的LSTM的输出，编码文本和图像的特征），从而实现特征之间的高效融合。在该计算方法中，两个特征$X, Y$之间的融合计算方法为：</p><p><img src="/2019/04/01/2019-03-28/2019-03-28-7.JPG" alt="formulation 1"></p><p>其中，$U, V$是参数，$k$是因子数，类似于multi-head attention中的head数量，为了使输出的稳定性更好，在本文中，作者加上了power normalization：$z \leftarrow sign(z)|z|^{0.5}$ 和L2 normalization：$z \leftarrow z/||z||$。以上整个过程记为$z = MFB(X, Y)$.</p><h3 id="Primary-Stage"><a href="#Primary-Stage" class="headerlink" title="Primary Stage"></a>Primary Stage</h3><p>首先是获取问题$m_t^q$ 和历史对话 $U$ 的交互关系，常用的attention操作，计算出两者之间的attention，然后使用历史对话得到问题的另一种表示，具体如下：</p><p>$$z_t^h=MFB_h(m_t^q, U), \ \alpha_t^h = softmax(\omega_{\alpha}^Tz_t^h), \ m_t^h = \sum_{i=1}^{t-1}(\alpha_{t,i}^hu_i)$$</p><p>通过该公式，就可以得到问题需要关注哪部分历史对话信息，接下来就是将$m_t^h$和问题向量$m_t^q$ 作拼接，然后使用相同的注意力计算方式得到那部分图像信息需要被关注，即$z_t^v = MFB_v([m_t^q; m_t^h], V)$. 这样就得到了利用需要关注的图像特征表示的问题$m_t^v$，最后就可以学习到文本和图像的联合表示 $e_t^p = MFB_e([m_t^q; m_t^h], m_t^v)$</p><p>得到了文本和图像的联合表示之后，本文将visual dialog中的答案也考虑到建模过程中，而不是简单的做分类，那么首先就是利用一个LSTM将每个候选答案$a_{t, i} \in A_t$ 进行编码，然后计算和图像文本联合表示之间的相似度</p><p><img src="/2019/04/01/2019-03-28/2019-03-28-8.JPG" alt="formulation 2"></p><p>其中$f_d$是一个一层的MLP，使用tanh作为激活函数，因为该函数计算出的是候选答案和联合表示之间的相似度，那么正确答案的相似度应该是最高的，因此本文使用了N-pair loss，同时为了解决不平衡问题，本文增加了一个缩放系数（temperature）去降低不平衡的影响，具体如下公式：</p><p><img src="/2019/04/01/2019-03-28/2019-03-28-9.JPG" alt="formulation 3"></p><p>其中$\tau \leq 1$, 如果候选答案被正确打分，并且该分数是小鱼ground truth的，那么改候选答案的损失$l_{t, i} = s_{t, i}^d - s_{t, gt}^d$ 就会小于0，而$\tau$ 就可以降低该答案对整个模型的影响，从而保证了模型的效果。</p><p>至此，该阶段就完成了目标，对所有的候选答案进行粗略打分。</p><h3 id="Synergistic-Stage"><a href="#Synergistic-Stage" class="headerlink" title="Synergistic Stage"></a>Synergistic Stage</h3><p>在第一阶段，由于打分函数或者其他问题，有些答案可能得到了不太合适的分数，因此该阶段的主要任务就是根据图像和问题对评分比较高的答案进行重新排序，（<em>即在第一阶段正确答案可能得到了比较高的分数，但不是最高的，如果直接根据第一阶段的分数来选出最后的答案的话，有可能会正确答案，但会错过最优选项，因此在该阶段对所有看似正确的答案进行重拍，从而选出最有答案</em>）。并且本文也通过实验证明了top 10的答案基本上覆盖了90%的正确答案，因此本文从答案集合$A_t$中选出了top N的结果，组成了新的答案集合$B_t = (b_{t,1}, …, b_{t,N})$</p><p>而在处理答案过程中，因为一些答案很短，例如NO, Black and Grey等，并没有什么具体的含义，因此这里是将每个答案和问题进行拼接，然后将其送个LSTM进行处理</p><p><img src="/2019/04/01/2019-03-28/2019-03-28-10.JPG" alt="formulation 4"></p><p>为了将该信息与历史对话以及图像信息进行联合建模，本文使用了和第一阶段相同的方法进行处理，但不同的是这里直接使用了第一阶段计算出来的历史对话加权向量表示，去计算对图像的注意力参数</p><p><img src="/2019/04/01/2019-03-28/2019-03-28-11.JPG" alt="formulation 5"></p><p>然后就是使用相同的注意力处理方式，对候选答案$b_{t,j}$ 进行加权表示，得到了需要关注的图像特征表示$m_{t, j}^r$，最后对这些内容做融合，$e_{t, j}^r = MFB_r([m_{t, j}^b; m_t^h], m_{t, j}^r)$，该向量表示的是和图像，问题以及历史对话进行协同建模之后的答案向量表示，最后将该向量表示送给一个一层的MLP进行打分，</p><p><img src="/2019/04/01/2019-03-28/2019-03-28-12.JPG" alt="formulation 6"></p><p>最后的最后，就是使用一个softmax函数进行分类，最合适的答案将拥有最高的概率。</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>本文是在<a href="https://visualdialog.org" target="_blank" rel="noopener">Visual Dialog V1.0</a>数据上进行相关实验的，评价标准使用的时一个鲁棒性更好的标准NDCG，而不是传统的检索指标，相关计算方式如下：</p><p><img src="/2019/04/01/2019-03-28/2019-03-28-13.JPG" alt="formulation 8"></p><p>接下来是实验结果：</p><p><img src="/2019/04/01/2019-03-28/2019-03-28-5.JPG" alt="result"></p><p>可以看出模型在几乎所有评价标准上都取得了最好的表示，当然直接看结果不够直观，本文也提供了一些case study，如下图：</p><p><img src="/2019/04/01/2019-03-28/2019-03-28-6.JPG" alt="case study"></p><p>从结果上看，第一阶段虽然会选出来正确的结果，但是经过第二阶段之后，模型能够选出来更符合情境的答案，不仅仅是对问题的简单回答，而且包含了一些细节，这是非常有意思的一点，感觉这部分值得深入做一做，不仅仅说能够提供正确答案，同时还能提供对应的细节信息，从而使答案更局有说服力。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本文聚焦于Visual Dialog任务中的答案上，提出了模型不仅仅需要给出正确的答案，还要给出更符合情境的答案，细节决定成败，尤其是在对话任务中，单单一个正确的答案并不能维持或者引导整个对话的进行，还需要有相应的细节信息，这样不仅能够使回答更具有说服力，而且为对话的维持提供了更多的可能，因此也更吸引人，本文的这个motivation还是非常有意思的，赞一个</p>]]></content>
      
      <categories>
          
          <category> paper_reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Image </tag>
            
            <tag> Visual Dialog </tag>
            
            <tag> VQA </tag>
            
            <tag> Sentence </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>DRr-Net: Dynamic Re-read Network for Sentence Semantic Matching</title>
      <link href="/2019/03/20/2019-03-20/"/>
      <url>/2019/03/20/2019-03-20/</url>
      <content type="html"><![CDATA[<blockquote><p>Title: DRr-Net: Dynamic Re-read Network for Sentence Semantic Matching</p><p>Authors: Kun Zhang, Guangyi Lv, Linyuan Wang, Le Wu, Enhong Chen, Fangzhao Wu, Xing Xie</p><p>Link: <a href="https://www.aaai.org/Papers/AAAI/2019/AAAI-ZhangKun.5147.pdf" target="_blank" rel="noopener">https://www.aaai.org/Papers/AAAI/2019/AAAI-ZhangKun.5147.pdf</a></p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>语义理解与语义表征一直是自然语言理解中基础但十分关键的一个内容，得益于大规模的数据和先进的深度学习技术，机器在具体的任务上的表现越来越接近人类表现，这其中注意力机制（Attention Mechanism）扮演着一个不可或缺的角色，它可以帮助模型选择出句子中对语义表达最重要的信息，从而生成一种更好地语义表示向量，但是大多数的方法都是一次性的选择出所有重要的信息，该方法明显是不够合理的。事实上，人在阅读的时候，会根据已学习到的信息去动态选择需要关注的内容。更具体的，认知心理学实验发现人在阅读的时候有两个特点：1）<strong>人对句子中的词序其实并没有很敏感，即使打乱顺序我们依然能够理解</strong>; 2) <strong>人在阅读时候倾向于阅读多遍关键信息，从而实现对句子语义的精确理解</strong>，如下图的例子：</p><p><img src="/2019/03/20/2019-03-20/2019-03-20-1.JPG" alt="眼动仪结果及乱序的句子"></p><p>左图是利用眼球追踪仪得到的注意力分布，可以看到有些词被反复阅读，而有些词并没有被阅读到。右图虽然词序是乱的，但是我们可以利用丰富的先验知识自动纠正这种错误，从而理解句子要表达的语义。基于以上的两个现象，本文提出了一种<strong>Dynamic Re-read</strong>机制，通过对重点内容的自动选择和反复阅读，从而实现对语义的精确理解。</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>首先是模型框架</p><p><img src="/2019/03/20/2019-03-20/2019-03-20-2.JPG" alt="model structure"></p><p>模型可以大致分为三个模块，</p><ul><li>Input Embedding：利用充足的特征信息对输入的每个词和每个句子进行编码</li><li>Dynamic Re-read Mechanism：每一步只关注一个最重要的词，并且反复对重要的内容进行理解</li><li>Label Prediction：对输入的两个句子之际的关系进行分类</li></ul><p>接下来，文中从这三个方面对整个模型进行详细分析</p><h3 id="Input-Embedding"><a href="#Input-Embedding" class="headerlink" title="Input Embedding"></a>Input Embedding</h3><p>对词的编码是一个常规操作，文本使用了预训练的词向量（glove 840d），字符级别的embedding，以及一些人工特征，然后将其拼接起来，为了更好地表示每个词的语义信息，本文将其通过一个两层的高速网络（Highway Network），最终得到每个词的语义表示${a_i | i = 1,2, … , l_a}$和${b_j|j=1,2,…,l_b}$</p><p>众所周知，人类拥有丰富的先验知识，因此可以利用这些知识直接选择出句子中对语义表示重要的词，但这对模型而言是非常难的，因此，模型首先要做的就是对整个句子的全面理解，尽可能多的了解句子信息，这样才能为更好的理解句子语义奠定基础。为了实现这个目的，本文设计了一种修改版的Stack-RNN，它将$（l-1）$层的输入和输出拼接起来，作为$l$层的输入，通过这种类残差的结构，模型就能够保留所有的信息，该过程可以形式化为：</p><p><img src="/2019/03/20/2019-03-20/2019-03-20-3.JPG" alt="formula1"></p><p>其中，$H_l$表示$l$层的RNN，这里使用的GRU作为RNN的基本单元。接下来要做的就是如何压缩这些词的向量表示，从全局的角度对句子语义进行向量表示。得益于自注意力机制（Self-attention），本文在Stack-RNN的最后输出上做self-attention，从而抽取对语义表示重要的信息，将这些信息加权求和，得到句子的全局语义表示，在文中，作者称之为original sentence representation，该过程可以由如下表示：</p><p><img src="/2019/03/20/2019-03-20/2019-03-20-4.JPG" alt="self-attention"></p><h3 id="Dynamic-Re-read-Mechanism"><a href="#Dynamic-Re-read-Mechanism" class="headerlink" title="Dynamic Re-read Mechanism"></a>Dynamic Re-read Mechanism</h3><p>该模块主要是受人在阅读时的习惯启发而设计出来的，人在阅读时：1）<strong>人对句子中的词序其实并没有很敏感，即使打乱顺序我们依然能够理解</strong>; 2) <strong>人在阅读时候倾向于阅读多遍关键信息，从而实现对句子语义的精确理解</strong>。因此该模块要实现的目标就是如何根据已学习到的知识在每一步选择一个重要的词进行理解，以及这些重要的词该如何处理。首先，整体的过程可以形式化为：</p><p><img src="/2019/03/20/2019-03-20/2019-03-20-5.JPG" alt="Dynamic Re-read"></p><p>即先通过一个选择函数，在整个输入序列中选出第$t$步的输入，然后将该输入送给一个GRU，而最后重要信息表示向量使用的是GRU的最后一个输出状态，因为GRU的在每个时刻的输入是动态变化的，因此作者将其称之为Dynamic。从该公式可以看出选择函数需要的输入有三个：a句子的整个序列，动态RNN的前一个时刻的隐层状态，b句子的全局语义表示，因为该模型针对的是句子语义匹配任务，因此作者将b句子的全局语义表示作为一个额外的context信息，这样模型就能更好的选择出符合上下文的语义信息。针对选择函数，作者利用了注意力机制来实现选择过程：</p><p><img src="/2019/03/20/2019-03-20/2019-03-20-6.JPG" alt="choose function"></p><p>相当于通过注意力机制在整个序列上计算出来当前时刻的重要程度分布，然后选择最大的权值对应的词，将这个词作为当前时刻的输入，然后将其送给GRU，但是，该过程有一个问题，最后一个公式选择出索引的操作是不可导的，这里作者做了一个trick，对softmax函数加了一个任意大的常数$\beta$，这样权值最大的就趋近于1了，而其他的权值就趋近于0了，通过这种近似的方法，实现了最后的选择操作，并保证可导。因此，上式可以修改为：</p><p><img src="/2019/03/20/2019-03-20/2019-03-20-7.JPG" alt="modification"></p><p>这就完成了整个动态选择的过程</p><h3 id="Label-Prediction"><a href="#Label-Prediction" class="headerlink" title="Label Prediction"></a>Label Prediction</h3><p>经过前两个模块，模型就生成了全局语义表示和局部重要表示，接下来作者通过启发式的拼接操作来整合这些信息，具体可以通过如下公式表示：</p><p><img src="/2019/03/20/2019-03-20/2019-03-20-8.JPG" alt="fusion"></p><p>这里是分别使用了不同的语义表示进行分类，为了是结果更准确，鲁棒性更好，作者设计了一个简单的加权方式将这两个结果进行融合，从而最终进行分类。</p><p><img src="/2019/03/20/2019-03-20/2019-03-20-9.JPG" alt="classification"></p><p>以上就是模型的整体结构。</p><p>该模型的创新点主要集中在Dynamic Re-read机制的设计上，作者通过模仿人的阅读习惯，每次只选择一个重要的词，然后对这些重要的词进行反复阅读，从而实现对句子语义的准确理解。</p><h2 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h2><p>首先是实验结果图</p><p><img src="/2019/03/20/2019-03-20/2019-03-20-10.JPG" alt="experiment result"></p><p>作者在两个任务，三个数据集上进行了测试，从实验结果上看，模型取得了非常不错的效果。同时为了验证模型的有效性，作者还对每个模块进行了验证，从结果上看，这其中最重要的还是全局的语义表示，重要信息的局部表示是在最好的结果上进行了提升，其重要程度要弱于全局语义表示。个人推测人在阅读时可以利用丰富的先验知识去直接挑选重要信息，而这对模型来说就有些困难了，他需要首先做的是对句子信息的整体把握，否则很难取得很好的效果。如果单纯只挑重要的信息的话，模型很难对句子语义有一个全面的表达。所以全局的语义表示对模型而言还是非常重要的。</p><p>为了更好地展示模型的能力，作者同时也进行了一些case study，如下图：</p><p><img src="/2019/03/20/2019-03-20/2019-03-20-12.JPG" alt="case study"></p><p>从结果上看，模型确实选择出了非常重要的词，但同时模型也在重复这些词，有时候甚至只重复一个词，最终导致模型得出错误的结论，这可能跟动态选择的过程是一个非监督的过程有关，这点还是值得仔细思考的。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本文通过利用人的阅读习惯，将注意力机制一次性选择所有重要的信息的机制修改为根据上下文动态选择重要的信息，并通过全局的表示和局部的表示对句子语义进行更全面的理解表示，是一个非常有意思的工作。按照作者的思路，在相关的领域，例如semantic matching， Visual Question Answering等方面都可以进行一些尝试，这是一个很值得研究的工作。</p>]]></content>
      
      <categories>
          
          <category> paper_reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sentence matching </tag>
            
            <tag> NLI </tag>
            
            <tag> attention mechanism </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Image-Enhanced Multi-Level Sentence Representation Net for Natural Language Inference</title>
      <link href="/2019/03/19/2019-03-19/"/>
      <url>/2019/03/19/2019-03-19/</url>
      <content type="html"><![CDATA[<blockquote><p>Title: Image-Enhanced Multi-Level Sentence Representation Net for Natural Language Inference</p><p>Authors: Kun Zhang, Guangyi Lv, Le Wu, Enhong Chen, Qi Liu, Han Wu, Fangzhao Wu</p><p>Link: <a href="https://ieeexplore.ieee.org/abstract/document/8594899/" target="_blank" rel="noopener">https://ieeexplore.ieee.org/abstract/document/8594899/</a></p></blockquote><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>自然语言理解(Natural Language Inference)主要是用于分析两个句子（前提句p，假设局h）之间的语义推理关系，包括Entailment(可以从前提句的语义推理出假设句的语义)，Contradiction(不能从前提句的语义推理出假设句的语义)，Neutral(无法确定是否能从前提句推理出假设句)。在真实条件下，自然语言句子的语义时高度依赖情境信息的，相同的句子可能会因为情境信息的不同而表达出不同的语义信息，这也是自然语言句子本身语义的模糊性，二义性等特点导致的，就如下边的例子</p><p><img src="/2019/03/19/2019-03-19/2019-03-21-1.JPG" alt="one simple example"></p><p>如果单纯看两个句子，我们可能会因为<em>outside market</em>而推理出天气是<em>sunny day</em>，但实际上这也不是能够十分明确的；而加入了情境信息的话，情况就会有所不同，例如图A，那么两个句子之间的语义关系就是Entailment，但换成图B的时候，因为时间变成了晚上，那么两个句子之间的语义关系就成了Contradiction。而目前大多数的方法在对句子语义进行建模的时候都假设句子是独立于情境的，这明显是不够充分的，因此，本文从这个角度出发，提出了将图像信息融入到句子语义建模过程中，从而增强句子的语义理解，得到更加准确全面的语义表示。</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>首先是完整的模型图</p><p><img src="/2019/03/19/2019-03-19/2019-03-21-2.JPG" alt="model structure"></p><p>从整体上看，模型是一个3-level的结构，在每一层的结构中包含了5个组件</p><ul><li>Input Layer：整个模型的输入层，包括两个句子的词的输入，图像的特征输入</li><li>Embedding Layer：从不同的粒度对句子中的信息进行表示，分别是词级别，短语级别，句子级别</li><li>Image-Enhanced Unit Layer：图像增强单元层，利用图像信息对句子语义表示进行增强，从而得到句子语义更加全面准确的表示</li><li>Matching Layer：对得到的句子语义表示进行匹配操作</li><li>Classification Layer：对输入的两个句子进行语义推理关系分类</li></ul><p>首先对输入层进行简单描述，输入的信息包含图像和文本，针对图像，作者选择了VGG19模型进行特征抽取，选择最后一个卷积层的结果作为图像的特征表示；针对文本，作者利用了预训练的词向量(glove 840b)，字符级别的embedding，以及一些人工特征的拼接，作为文本中每个词的表示。接下来将对模型的几个结构进行详细分析</p><h3 id="Embedding-Layer"><a href="#Embedding-Layer" class="headerlink" title="Embedding Layer"></a>Embedding Layer</h3><p>和传统的直接拿embedding好的词向量作为模型的最终输入不同的是，本文首先从不同粒度对模型输入进行处理。这里为什么要这么处理呢？因为本文最终是要通过图像信息对句子的语义表示进行增强，而图像包含的信息要远远高于文本，并且它可能对应到文本的不同粒度上，可能是某个词，也可能是某个短语，另一方面，对文本的多粒度建模可以更好地对句子语义进行分析，因此，本文首先对原始的词向量进行一维卷积（使用unigram, bigram, trigram），得到了句子的短语表示，需要说明的是，这里的短语并不是真正的短语，只是一个局部的特征。在此基础上，使用了GRU对短语级别的表示进行处理，这样就知道了一个全局信息，这里作者称之为句子级别的表示。</p><p>在接下来的内容中，以词级别的作为例子，剩下的两个粒度信息使用的是相同的结构。</p><h3 id="Image-Enhanced-Unit"><a href="#Image-Enhanced-Unit" class="headerlink" title="Image-Enhanced Unit"></a>Image-Enhanced Unit</h3><p>首先是该结构的详细图解</p><p><img src="/2019/03/19/2019-03-19/2019-03-21-3.JPG" alt="IEU"></p><p>该结构的输入为三部分，一个附加的情境信息$v$，图像的特征表示$C$，以及文本序列的表示$P^{lx}$，这里使用的词级别的前提句的序列表示矩阵作为例子。首先要做的就是利用图像的特征表示来从另一个角度表示文本中的每个词，很自然的，co-attention方法是首选：</p><p><img src="/2019/03/19/2019-03-19/2019-03-21-4.JPG" alt="co-attention"></p><p>通过该操作，模型就可以利用图像中的相关信息对文本的语义表示进行增强，相当于从另外一个角度对每个词进行了表示，那么接下来最简单的方法就是把两个结果拼接起来，但这样会让模型的参数规模变大，并且是否可以将来自图像的信息表示和来自文本的信息表示同等对待也值得思考，在本文中，作者仿照GRU的结构，对这两部分的表示进行了一个融合，从而更好的利用这部分的信息：</p><p><img src="/2019/03/19/2019-03-19/2019-03-21-5.JPG" alt="fusion"></p><p>融合之后就得到了每个词的增强表示，接下来需要做的就是如何将这些信息融合，得到句子的语义表示向量，这里作者使用了self-attention机制，并同时使用了最大池化操作（max-pooling），这样对句子语义表示重要的信息会被选择出来，同时对语义表示最重要的信息也会被选择出来进行重点突出，通过这种方式，就可以得到句子语义全面的一个表示：</p><p><img src="/2019/03/19/2019-03-19/2019-03-21-6.JPG" alt="pooling"></p><p>从公式中可以看出，作者最后将这三部分内容拼接起来，作为最终的句子的语义表示向量。同样的操作也会放到假设句子上，这样就得到了两个句子的增强语义表示。需要强调的是这里的$v$，这是一个被称之为inference vector的向量，会在下一部分详细分析</p><h3 id="Matching"><a href="#Matching" class="headerlink" title="Matching"></a>Matching</h3><p>这部分就相对比较简单了，将得到的两个句子的语义向量表示进行启发式的匹配操作</p><p><img src="/2019/03/19/2019-03-19/2019-03-21-7.JPG" alt="matching"></p><p>这里，直接拼接可以保留所有的信息，相减操作可以指明推断方向，同时是一种最朴素的差异判断方法，而点乘可以认为是一种最简单的相似度计算方法，通过这样的操作，就可以得到两个句子之间推理关系的一个向量表示，正如上一节所描述的，IEU单元需要有这样一个inference vector作为输入，这里作者是将上一层的inference vector送给当前层，这样模型在短语级别推理时会指导词级别做出分类的理由，这样不同粒度之间也建立了信息流动，从而随着粒度的加深，模型也能够更加准确的进行语义推理关系的分析和判断。</p><p>最后就是分类层了，通过一个简单的分类器进行分类就可以了。因为这是一个三层的结构，每一层都会分类，因此作者在训练时要求模型每一层都能分对，也就是损失函数中有这三层的交叉熵损失函数。而在预测的时候使用的是最后一层的输出结果。</p><p>以上就是模型的整体结构，一个多粒度的图像增强结构，还是很有意思的。</p><h2 id="Experiment-Result"><a href="#Experiment-Result" class="headerlink" title="Experiment Result"></a>Experiment Result</h2><p>还是首先先上实验结果</p><p><img src="/2019/03/19/2019-03-19/2019-03-21-8.JPG" alt="experimental results"></p><p>从实验结果上看，该模型在完整的测试集上，以及更加严格的测试集上都能取得不错的效果，这也说明了模型的有效性，接下来是模型结构的有效性测试</p><p><img src="/2019/03/19/2019-03-19/2019-03-21-9.JPG" alt="ablation results"></p><p>从这个结果上看，只用单层的话，能看到的信息越多，效果越好，因此我们看到粒度越大，效果越好，并且只用词级别的表示的话，因为完全不知道句子的结构，所以结果和随机猜没有太大差别；当考虑的粒度越多时模型的效果也越好，这也说明了多粒度的考虑确实有助于对句子语义的全面理解。</p><p>不过个人感觉这部分应该加一个如果没有图像信息的话，结果 会怎么样，相当于是对图像有效性的一个检测，这样实验部分就更加全面了。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>本文通过将图像信息引入到自然语言推理中，利用额外补充的信息对句子语义进行增强，从而更加准确的理解句子语义，思路非常新颖，而且这种多模态的建模方法个人感觉也是未来一个比较重要的研究方向。当然本文中也有一些地方还可以改进，例如图像的利用，图像的有效性分析等都值得思考。还是一篇很不错的文章。</p>]]></content>
      
      <categories>
          
          <category> paper_reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sentence matching </tag>
            
            <tag> NLI </tag>
            
            <tag> Image </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Neural Network for Named Entity Typing方法总结</title>
      <link href="/2018/07/04/2018-07-04/"/>
      <url>/2018/07/04/2018-07-04/</url>
      <content type="html"><![CDATA[<h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><ol><li>Neural Architectures for Fine-grained Entity Type Classification </li><li>Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss </li><li>Fine-grained Entity Typing through Increased Discourse Context and Adaptive Classification Thresholds </li></ol><h3 id="文章1"><a href="#文章1" class="headerlink" title="文章1"></a>文章1</h3><h4 id="模型图"><a href="#模型图" class="headerlink" title="模型图"></a>模型图</h4><p><img src="/2018/07/04/2018-07-04/2018-07-04-3.PNG" alt="model structure"></p><p>这是相对来说比较早的一篇利用神经网络来解决NET问题的工作，从模型中大致可以看出该模型的基本框架，1）预训练的词向量进行embedding，2）针对entity mention周边的context，分别使用不同的Bi-LSTM进行处理，并在其隐层状态上使用attention，最后得到context的向量表示，针对entity mention，直接使用平均的方法，将所有词的向量加起来，得到entity mention的向量表示，最后使用一个MLP进行分类。</p><h4 id="亮点"><a href="#亮点" class="headerlink" title="亮点"></a>亮点</h4><p>文章在考虑label的时候，使用了hierarchical label encoding的方法，具体来讲，fine-gained label具有先天的层次化特征，例如person/artist/actor，如果预测entity mention的label为actor，那么理论上是可以回溯上去的，得到person/artist，因此作者考虑将这些信息进行层次化联合编码，还是使用one-hot的表示方法，不过对应的label都会得到1，因此一个label中不只有一个1，然后乘以一个embedding矩阵，得到label的低维表示，然后将得到的低维表示作为最后分类层的参数，进行分类，这个就很有意思了，具体实现过程，可以参见下图：</p><p><img src="/2018/07/04/2018-07-04/2018-07-04-5.PNG" alt="hierarchical label encoding"></p><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><h3 id="文章2"><a href="#文章2" class="headerlink" title="文章2"></a>文章2</h3><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><ol><li><p>噪声问题</p><p>NET任务有一个很大的问题，那就是数据的问题，目前常用的几个数据集，FIGER，OntoNote，WiKi基本上都是通过如下步骤生成的：</p><ol><li>在句子或者文档中使用NER等工具标记entity mention</li><li>将得到的entity mention和知识图谱中的entities做关联</li><li>找到知识图谱中每个entity的类型，将其作为目标entity mention的候选类型</li></ol><p>从这个流程中就可以看出来一个问题，得到的type类型有很高的噪声，而且针对一句特定的句子或文档，这些候选类型中后很多是不符合情境的，也就是说上面的方法在做标注的时候是无法考虑情境的，例如下图</p><p><img src="/2018/07/04/2018-07-04/2018-07-04-4.PNG" alt="example"></p><p>如果只考虑句子s1，那么得到的type就应该是person/coach，如果只考虑句子s2，那能得到的type就应该是athlete，</p></li><li><p>标注过细问题</p><p>还是上图的例子，如果只考虑句子s3，那么得到的就只能是person，但是由于这个人是个名人，因此在知识图谱上会给他标记上很多很细的type，这会导致在训练的时候，模型会更喜欢分这些很细的类别，而不是一般的，但是符合句子情境的类别。</p></li></ol><p>针对以上问题，作者提出了自己的模型，关键的地方在于作者设计的loss function</p><h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p><img src="/2018/07/04/2018-07-04/2018-07-04-2.PNG" alt="model structurue"></p><p>这个模型还是很简单的，相对于上一个模型，在该模型中，作者将考虑的重点放在了entity mention上边，具体流程如下：</p><ol><li>embedding layer将句子中的每个词进行embedding</li><li>针对entity mention，a）将所有词的向量做平均，得到 一个表示；b）在entity mention的基础上左右各扩展一个词，然后将得到的这个短语通过一个LSTM，然后将最后一个状态作为entity mention的另一种表示</li><li>针对context，不再是考虑使用不同的LSTM来处理两边的内容，而是处理整个句子，然后在隐层状态上做attention，最后得到一个加权和作为context的表示，这里的attention是在整个句子上做self-attention。</li><li>将得到的三个向量作拼接，然后将结果送给分类器进行分类</li></ol><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>针对数据上存在的两个问题，作者在损失函数上做了一些巧妙地调整。</p><ol><li><p>out-of-context noise</p><p>针对这个问题，作者在计算损失时，不是考虑所有类别上的损失，即传统的loss function</p><p>$$J(\theta) = \frac{1}{N}\sum_{i=1}^{N}y_ilog(p(y_i|m_i, c_i)) + \lambda||\theta||^2, \quad \tag{1}$$</p><p>在这个公式里，$y_i$是一个向量。作者只考虑其中概率最大的类别，即$y_i^{‘} = argmax_{y\in y_i^t}p(y|m_i, c_i)$，这样损失函数就变成了</p><p>$$J(\theta) = \frac{1}{N}\sum_{i=1}^{N}(y_i^{‘}log(p(y_i^{‘}|m_i, c_i)))+ \lambda||\theta||^2, \quad \tag{2}$$</p><p>作者这么做，是假设在训练集中获得概率最高的类别是真正符合情境的类别，而概率比较低的类别就和情境不太搭了，这其实也是有一定 道理的，而且很巧妙地将情境信息的作用引入进了模型。</p></li><li><p>overly-specific noise</p><p>针对这个问题，作者提出了一种Hierarchcical Loss Normalization的损失函数，因为越细粒度的类别，他们所占的比重应该是不一样的，例如如果选择了athlete这个标签，那么较粗的粒度肯定要选择person，而不是location之类的，而传统的loss function在优化时，就是将他们平等对待的，因此作者希望损失函数能够更少的惩罚那么相关联的那些标签，即</p><p>$$p^{‘}(y|m,c) = p(y|m, c) + \beta\sum_{t\in\Gamma}p(t|m,c), \quad \tag{3}$$</p><p>其中，$\Gamma$就是在类型路径上属于$y$的祖先节点，这样对类型进行分类处理，这样在最后的分类预测时就可以降低不同类别的噪声的影响了。</p></li></ol><p>这就是这篇文章的大体框架，主要是在噪声处理上一些措施，感觉还是值得借鉴的</p><h3 id="文章3"><a href="#文章3" class="headerlink" title="文章3"></a>文章3</h3><p>这篇文章也是针对第一篇文章进行的一些改进，主要在以下的几个地方：</p><ol><li>context不再是独立处理</li><li>针对context的attention，要考虑到和entity mention之间的对齐关系</li><li>增加了文档级别的context</li><li>不再考虑人工特征，并通过实验证明人工特征加进去对模型的提升并不大</li></ol><h4 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h4><p><img src="/2018/07/04/2018-07-04/2018-07-04-1.PNG" alt="model structure"></p><p>从模型图里可以看出来整个模型的框架，</p><ol><li>entity encoder：相同的方法，首先embedding，然后进行进行平均，得到entity的表示</li><li>sentence-level context：就是entity所处的句子，将其通过一个L层Bi-LSTM，然后利用隐层状态和entity表示之间的dot product计算attention，最后对所有的隐层状态做加权，得到句子级别的context表示</li><li>document-level context：这部分相对简单一些，做着先使用了一个预训练好的模型（gensim.doc2vec）得到文档的表示，然后通过一个多层感知机，得到文档级别的context表示</li><li>拼接起来，得到输入的特征表示，在此基础上，将类别也进行embedding，最后通过一个逻辑回归得到该entity输入每个类别的概率，</li><li>这点独立写出来，算是作者的一个亮点，作者不是使用一个统一的阈值来决定eneity mention属于哪个类别，而是使用了 一种自适应的阈值，每个不同的类别都有一个属于自己的阈值，该阈值是通过模型学出来的，然后预测出来的概率只有超过该阈值才能说属于该类别。</li></ol><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>作者对结果进行了一些分析，得到以下两点：</p><ol><li>人工构建的特征作用并不大，可能是因为这些人工特征大多都是关于语法或者主题的一些信息，而这些信息都通过注意力机制和文档级别的context已经得到了，因此再补充这些信息的意义不大</li><li>对类别进行层次化编码的效果也很小，并做作者通过一些例子展示，说明entity mention是依赖情境的，属于不同粗粒度的类别可能由于情境的原因而被标记为entity mention的类别，也就是说类别的层次化信息其实意义不太大，（对这点我持保留态度）</li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>从这三篇文章中基本上可以看到目前使用神经网络做NET的一些基本方法以及面对的一些问题，个人感觉可以从以下几点考虑新的方法：</p><ol><li>entity mention的表示</li><li>context的选择及表示</li><li>label的表示，包括噪声的处理，层次化信息的运用等，</li></ol><p>目前基本上只想到这些，感觉从这个地方搞起来还是有很多可以做的东西，♪(＾∀＾●)ﾉ</p>]]></content>
      
      <categories>
          
          <category> paper_reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NET </tag>
            
            <tag> context， neural network </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>What Value Do Explicit High Level Concepts Have in Vision to Language Problems?</title>
      <link href="/2018/06/30/2018-06-30/"/>
      <url>/2018/06/30/2018-06-30/</url>
      <content type="html"><![CDATA[<blockquote><p>Title: What Value Do Explicit High Level Concepts Have in Vision to Language Problems?</p><p>Authors: Qi Wu, Chunhua Shen, Lingqiao Liu, Anthony Dick, Anton van den Hengel</p><p>Link: <a href="https://arxiv.org/abs/1506.01144" target="_blank" rel="noopener">https://arxiv.org/abs/1506.01144</a></p></blockquote><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Vision-to-Language(V2L)主要是研究图像和文本的联合建模，利用图像为文本提供丰富的辅助信息，利用文本为图像提供高层次的抽象信息。该主题有着越来越多的应用场景，例如Image Captioning，Visual Question Answering, Visual Dialog等等。目前常用的方法就是利用CNN来处理图像，利用RNN来处理文本，然后直接整合或者通过Attention Mechanism来整合。那么，这些方法是否真正理解了图像的意义，或者说利用CNN的方法是否真的获取到了图像的高层次的抽象信息。针对这个问题，本文在CNN-RNN的基础上提出了一种利用图像高层次语义的方法，并在多个任务上验证了其有效性。</p><h2 id="模型框架"><a href="#模型框架" class="headerlink" title="模型框架"></a>模型框架</h2><p>首先展示模型的基本框架</p><p><img src="/2018/06/30/2018-06-30/2018-06-30-1.PNG" alt="model structure"></p><p>从图中我们可以看出模型的大致结构</p><ol><li>利用预训练的模型，经过transfer learning之后得到multi-label的模型结构，并将图像输入抽取到一个对应的属性向量</li><li>将得到的图像属性应用到不同的V2L任务当中去。</li></ol><p>那么很重要一点就是如何将图像用一个属性向量表示，这部分，在本文中，作者称其为属性预测器，接下来就来详细了解一下这个属性预测器。</p><h2 id="属性预测器（Attribute-Predictor）"><a href="#属性预测器（Attribute-Predictor）" class="headerlink" title="属性预测器（Attribute Predictor）"></a>属性预测器（Attribute Predictor）</h2><ol><li><p>属性字典：</p><p>既然要使用高层次的抽象表示来表示图像的信息，那么最直接的方法就是和自然语言建立联系，因此作者首先建立了一个属性字典。为了保证得到的词和图像之间紧密相联，作者使用了image captioning中的图像和描述，然后抽取频率最高的c个词，为了保证效果同时降低属性字典的大小，该字典是时态，单复数不敏感的，也就是说“dogs”和“dog”是同一个属性，最后就建立了一个大小为256的字典。有了这个字典，就可以利用图像的描述将图像和属性字典联系起来了。</p></li><li><p>多标签分类：</p><p>为了将图像和属性联系起来，作者将这个问题看作是多标签分类问题，具体方法如下图：</p><p><img src="/2018/06/30/2018-06-30/2018-06-30-2.PNG" alt="multi-label classification"></p><p>从图中我们大致可以看清楚整个流程，作者使用了预训练好的VGG模型作为模型的初始化参数，然后在该模型方法image captioning数据集MS COCO上进行fine-tune，这一步，作者将最后一层改为一个c分类的模型，c就是属性字典的大小。在这里，作者使用了element-wise logistic loss function。也就是说在图像的描述中，如果存在某个属性$y_i \in c$，那这个$y_i=1$，否则$y_i = 0$，然后损失函数就可以写为</p><p>$$J = \frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{c}log(1+exp(-y_{ij}p_{ij})), \quad \tag{1}$$</p><p>在fine-tuning过程中，只有VGG的最后两个全连接层和模型的最后一层进行训练，其他层保持不变。</p><p>在此基础上，考虑到不同的属性关注的仅仅是图像中的某一个区域，因此作者首先使用了一种normalized cut的算法将图像切分为m个聚类，然后每个聚类中的图像块在区域划分算法中评分比较高的top-k会被挑选出来送给CNN，这个CNN就是上一个fine-tuning之后的结果。同时为了更好的表示图像，作者也将整幅图像送给该CNN，这样就有了mk+1属性表示向量，最后作者使用一个maxpooling操作，得到图像的最终属性表示向量$V_{att}(I)$。这个是一个十分重要的表示</p></li></ol><h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>这部分相对来说和一般的做法差别并不大，作者分别在Image captioning，VQA-single word和VQA-sentence三个任务上进行验证，首先看如何将这个属性向量插入</p><p><img src="/2018/06/30/2018-06-30/2018-06-30-3.PNG" alt="language generators"></p><p>蓝色虚线是对比方法，红色实线是图像的属性向量，这个还好理解的。Image captioning模型中使用属性向量作为-1时刻的输入（并不是直接作为隐层状态的初始化，经过了一个线性变换）；VQA-Single-word任务中也是一样的操作；VQA-sentence任务中就是将其作为生成器的初始化输入。</p><p>这部分相对来说比较简单，个人感觉作者可能认为在图像处理阶段已经将图像使用高层次的抽象信息表示了，在这里就是自然语言了，有了这个中间桥梁，图像和对应的文本之间就更好关联起来了，因此一般都将其放到了语言模型的初始化部分。当然，也正是因为这样，语言模型处理的可以认为基本上都是文本信息，因此就会有比较好的表现效果。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>照例还是直接上实验结果图：</p><p><img src="/2018/06/30/2018-06-30/2018-06-30-4.PNG" alt="experiment results"></p><p><img src="/2018/06/30/2018-06-30/2018-06-30-5.PNG" alt="experiment results2"></p><p>作者使用的数据集主要是MS COCO，Flickr8k和Flickr30k，实验结果证实了作者的方法确实十分有效，作者也在VQA上进行了一些验证，这里就不再展示了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>传统的V2L任务中，大家一般都是利用预训练好的图像模型来抽取图像特征，然后将其应用到语言模型中，作者在这里给我们提供了一种新的思路，利用已有的信息，为图像和文本之间建立一个桥梁，这样直观的信息表示和抽象的信息表示能够更好的融合在一起，从而为更好的理解语义，理解图像提供支撑。我自己还有一个想法：目前使用的预训练模型一般都是在ImageNet上进行训练的，而ImageNet主要是一个分类图像数据集，因此这些模型抽取的特征可能更多的服务于分类任务，但将其利用到V2L时是不是就会有一些信息偏差呢，是不是利用比如说做目标检测的图像预训练模型会有不一样的效果呢？值得思考↖(^ω^)↗</p>]]></content>
      
      <categories>
          
          <category> paper_reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> V2L </tag>
            
            <tag> Image Captioning </tag>
            
            <tag> Language Model </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information</title>
      <link href="/2018/06/30/2018-06-29/"/>
      <url>/2018/06/30/2018-06-29/</url>
      <content type="html"><![CDATA[<blockquote><p>Title: Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information</p><p>Authors: Seonhoon Kim, Jin-Hyuk Hong, Inho Kang, Nojun Kwak</p><p>Link: <a href="https://arxiv.org/abs/1805.11360" target="_blank" rel="noopener">https://arxiv.org/abs/1805.11360</a></p></blockquote><p>句子匹配（Sentence Matching）是自然语言理解任务中一个非常重要的任务，例如Natural Language Inference，Paraphrase Identification，Question Answering等都可以归属于这个任务。这个任务主要就是理解句子语义，理解句子之间的语义关系。因此如何去表示这些内容就变得十分重要了。为了更好的利用原始特征信息，作者参考DenseNet，提出了一种densely-connected co-attentive recurrent neural network模型，该模型最突出的地方就是可以从最底层到最顶层一直保留原始信息以及利用co-attention得到的交互信息。接下来，就对文章进行详细了解</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>首先是模型图：</p><p><img src="/2018/06/30/2018-06-29/2018-06-30-6.PNG" alt="model structure"></p><p>不得不说，这个图还是很粗糙的，一点都不够精致，但模型的基本单元以及整体框架已经完全包含进去了，我们姑且用这个图对模型进行分析吧</p><h3 id="输入层"><a href="#输入层" class="headerlink" title="输入层"></a>输入层</h3><p>自然语言的任务首先就是输入层，对每个词的one-hot表示进行embedding，</p><p>$$e_{pi}^{tr}  = E^{tr}(p_i), \\ e_{pi}^{fix}  = E^{fix}(p_i), \\ c_{p_i} = Char-Conv(p_i), \\ p_i^w = [e_{pi}^{tr}; e_{pi}^{fix}; c_{p_i}; f_{p_i}], \quad \tag{1}$$</p><p>这几个公式很好理解，首先作者将词的embedding分为两部分，一部分参与训练，即$E^{tr}$，另一部分是固定不动的，即$E^{fix}$，然后就是词级别的表示char-Conv，以及一些exact match的匹配特征，主要是a中的每个词是否在b中有对应的词，然后将这些表示拼接起来，就得到了每个词的最后表示$p^w_i$。</p><h3 id="密集连接层"><a href="#密集连接层" class="headerlink" title="密集连接层"></a>密集连接层</h3><p>在这一层，作者收DenseNet启发，使用了密集连接和RNN结合的方法来实现对对句子的处理。首先$h_t^l$表示的是第l层的RNN的第t的隐层状态，</p><p>$$h_t^l = H_l(x_t^l, h_{t-1}^l), \quad x_t^l = h_t^{l-1}, \quad \tag{2.1}$$</p><p>$$h_t^l = H_l(x_t^l, h_{t-1}^l), \quad x_t^l = h_t^{l-1} + x_t^{l-1}, \quad \tag{2.2}$$</p><p>$$h_t^l = H_l(x_t^l, h_{t-1}^l), \quad x_t^l = [h_t^{l-1}, x_t^{l-1}], \quad \tag{2.3}$$</p><p>式2.1是传统的多层RNN的结构，前一层的RNN的 隐层状态作为当前层的输入，然后就是RNN的计算方式，式2.2借鉴了残差网络，当前层的输入不仅包含了前一层的隐层状态，同时包含了前一层的输入，但他们是相加的方式，作者认为这种相加的形式很可能会阻碍信息的流动，因此借鉴DenseNet，作者使用了拼接了方式，这样不仅保留了两部分信息，同时拼接方法也最大程度的保留了各自的独有信息。但这就有一个问题了，多层的RNN的参数就不一样了，因为拼接的方式导致了每一层输入对应的参数规模是在不断变大的，这样就不能做的很深了。</p><h3 id="密集连接注意力"><a href="#密集连接注意力" class="headerlink" title="密集连接注意力"></a>密集连接注意力</h3><p>因为句子匹配考虑的两个句子之间关系，因此需要建模两个句子之间的交互，目前来说，注意力机制是一种非常好的方法，因此作者在这样也使用了注意力机制，</p><p>$$a_{p_i} = \sum_{j=1}^{J}\alpha_{i,j}h_{q_j}, \\ \alpha_{i,j} = \frac{exp(e_{i,j})}{\sum_{k=1}^Jexp(e_{i,k})}, \quad e_{i,j} = cos(h_{p_i}, h_{q_j}), \quad \tag{3.1}$$</p><p>这个就是传统的co-attention计算方法，计算两个序列之间的在每个词上的对应关系，不过作者这里比较粗暴，直接使用了余弦相似度来计算每两个词之间的相似，这里也可以使用一个简单的MLP来计算。有意思的地方在下边</p><p>$$h_t^l = H_l(x_t^l, h_{t-1^l}), \quad x_t^l = [h_t^{l-1}, \alpha_t^{l-1}, x_t^{l-1}], \quad \tag{3.2}$$</p><p>这个就很有意思了，我们传统的做法是得到每个词在对方句子上的概率分布之后，使用对方句子中每个词向量的加权和作为当前词的向量表示，而这里作者直接使用了计算出来的权值分布，将其作为一个特征引入到当前层的输入当中，这个感觉还是很有意思的。</p><h3 id="瓶颈处理层"><a href="#瓶颈处理层" class="headerlink" title="瓶颈处理层"></a>瓶颈处理层</h3><p>正如前边提到的，这种dense连接方式直接导致的一个问题就是随着模型的加深，参数量会变的越来越多，这样最后全连接层的压力就会特别大。因此作者在这里使用了一个AutoEncoder来解决这个问题。AutoEncoder可以帮助压缩得到的巨大向量表示，同时可以保持原始的信息。这个操作还是很不错的。</p><h3 id="分类层"><a href="#分类层" class="headerlink" title="分类层"></a>分类层</h3><p>这是处理两个句子关系常用的一种匹配方法，作拼接，相减，点乘，不过作者在这里也是用了相减的绝对值，然后将最终拼接的向量通过一个全连接层，然后根据任务进行softmax分类，我个人做过实验，相减的效果要好于相减的绝对值，因为相减不仅可以表示差异，同时可以表明信息流方向，而相减的绝对值就更专注于差异了，两个都用应该是效果比只用一个好的。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>照例，上图，作者在NLI任务和Question Pair两个任务上进行了模型验证，效果当然是十分不错的。</p><p><img src="/2018/06/30/2018-06-29/2018-06-30-7.PNG" alt="experiment results"></p><p><img src="/2018/06/30/2018-06-29/2018-06-30-8.PNG" alt="experiment results"></p><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>这篇文章主要集中在句子匹配任务上，将DenseNet的一些想法引入到了stack RNN中，还是可以给人一些灵感的，比如说从残差连接到DenseNet，比如说注意力权值的使用方法，比如说利用AutoEncoder来压缩向量，这些还是十分值得学习的。♪(＾∀＾●)ﾉ</p>]]></content>
      
      <categories>
          
          <category> paper_reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sentence matching </tag>
            
            <tag> NLI </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Neural Architectures for Fine-grained Entity Type Classification</title>
      <link href="/2018/06/20/2018-06-20/"/>
      <url>/2018/06/20/2018-06-20/</url>
      <content type="html"><![CDATA[<blockquote><p>Title: Neural Architectures for Fine-grained Entity Type Classification </p><p>Author: Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, Sebastian Riedel</p><p>Link: <a href="https://arxiv.org/pdf/1606.01341" target="_blank" rel="noopener">https://arxiv.org/pdf/1606.01341</a></p></blockquote><h2 id="包含文章"><a href="#包含文章" class="headerlink" title="包含文章"></a>包含文章</h2><ol><li>An Attentive Neural Architecture for Fine-grained Entity Type Classification </li><li>Neural Architectures for Fine-grained Entity Type Classification </li></ol><p>这两篇文章是同一组作者做的，相关内容也比较类似，因此放到一起进行介绍</p><h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>Named Entity Typing (NET)主要是为实体表示（entity mention）标记其类别的一个任务，输入一般是一个句子，包含情境和实体表示，一般用$[C_{-s}, C_{-s+1}, …, C_{-1}][w_1, w_2, …, w_n][C_1, C_2, …, C_s]$表示，输出的结果就是实体表示的类别，和传统的分类任务不同的地方在于，实体表示有可能是属于多个类别的（如果分类粒度比较细的话），而且这些类别之间也存在包含关系之类的，这点就比较复杂了，目前比较常用的做法是<em>首先根据输出概率，选择一个最大的作为entity mention的类别（保证至少有一个类别），然后设定阈值，大于阈值的都可以认为是该entity mention的类别。</em></p><h3 id="评价标准"><a href="#评价标准" class="headerlink" title="评价标准"></a>评价标准</h3><p>一般使用以下三个作为实验效果的评价标准：</p><ol><li><p>strict</p><p>$$Precision = Recall = \frac{1}{N}\sum_{i=1}^{N}\delta(\hat{T}_i = T_i), \quad \tag{0.1}$$</p></li><li><p>loose macro</p><p>$$Precision = \frac{1}{N}\sum_{i=1}^N\frac{|\hat{T}_i\cap T_i|}{|\hat{T}_i|}, \quad \tag{0.2}$$ </p><p>$$Recall = \frac{1}{N}\sum_{i=1}^N\frac{|\hat{T}_i\cap T_i|}{T_i}, \quad \tag{0.3}$$</p></li><li><p>loose micro</p><p>$$Precision = \frac{\sum_{i=1}^N|\hat{T}_i \cap T_i|}{\sum_{i=1}^{N}|\hat{T}_i|}, \quad \tag{0.4}$$</p><p>$$ Recall= \frac{\sum_{i=1}^N|\hat{T}_i \cap T_i|}{\sum_{i=1}^{N}|T_i|}, \quad \tag{0.5}$$</p></li></ol><h2 id="模型框架"><a href="#模型框架" class="headerlink" title="模型框架"></a>模型框架</h2><p><img src="/2018/06/20/2018-06-20/2018-06-20-2.PNG" alt="model structure"></p><p>该框架展示了基于神经网络的NET模型的基本结构，在本文中利用上图对该 框架进行简单介绍</p><h3 id="Entity-Mention表示"><a href="#Entity-Mention表示" class="headerlink" title="Entity Mention表示"></a>Entity Mention表示</h3><p>首先得到每个词的词向量，将entity mention中的所有词向量做平均，得到entity mention的表示，考虑到entity mention一般不是很长，所以该方法简单有效，当然也可以使用RNN之类的方法进行表示，</p><p>$$v_m = \frac{1}{M}\sum_{i=1}^Mu(m_i), \quad \tag{1}$$</p><p>其中，u就是将每个词表示为它的词向量表示。</p><h3 id="context表示"><a href="#context表示" class="headerlink" title="context表示"></a>context表示</h3><p>很容易理解，entity mention的语义是十分依赖其所存在的情境的，如果是在一个句子中的话，那么该情境信息就是上下文的词，在本文中，作者通过三种方法来处理这些词：</p><ol><li><p>和entity mention表示类似，采用和平均的方法</p><p>$$v_c = \frac{1}{C}\sum_{i=1}^{C}[u(l_i), u(r_i)], \quad \tag{2.1}$$</p></li><li><p>考虑到情境信息是一个序列关系，因此也可以采用LSTM来处理</p><p>$$h_i, s_i = lstm(u_i, h_{i-1}, s_{i-1}), \\ v_c = [\overrightarrow{h_C^l}, \overleftarrow{h_1^r}] \quad \tag{2.2}$$</p></li><li><p>单纯那最后一个状态进行拼接并不能有效利用LSTM中的信息，因此注意力机制派上了用场，这里采用了得是一种类似于self-attention的方法：</p><p>$$e_i^l = tanh(W_e[\overrightarrow{h_i^l}, \overleftarrow{h_i^l}]), \quad \tag{2.31}$$ </p><p>$$\widetilde{a}_i^l = exp(W_ae_i^l) , \quad \tag{2.32} $$</p><p>$$ a_i^l = softmax(\widetilde{a}_i^l) \quad \tag{2.33}$$</p><p>$$v_c = \sum_{i=1}^{C}a_i^l[\overrightarrow{h_i^l}, \overleftarrow{h_i^l}] + a_i^r[\overrightarrow{h_i^r}, \overleftarrow{h_i^r}], \quad \tag{2.34}$$</p><p>从这里我们可以看出是由各自的输入决定各自的权重，但在最后计算整体的权重正规化时同时考虑了左侧和右侧的权重，在这里将这两部分同时考虑，最后得到一个加权和作为最后的context表示。</p></li></ol><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>分别得到entity mention和context的表示之后，普遍的做法是直接将这两部分拼接起来，然后进行逻辑回归，</p><p>$$y = \frac{1}{1 + exp(-W_y[v_m, v_c])}, \quad \tag{3.1}$$</p><p>有了预测结果，考虑到这是一个分类任务，那么就可以使用交叉熵作为损失函数，</p><p>$$L(y, t) = \sum_{k=1}^K-t_klog(y_k) - (1-t_k)log(1-y_k), \quad \tag{3.2}$$</p><p>在这个损失函数中，K表示所有的类别数，t是预测出来的二值向量，即在每个类别上都要做一个二分类，根据之前的介绍，这部分需要这么做，因为他的分类结果是不一定的。</p><p>这就是基于神经网络的NET模型的大体框架，从我看到的几篇文章中，基本上都是用了这样的框架，只是在处理细节上略有不同，</p><h2 id="额外部分"><a href="#额外部分" class="headerlink" title="额外部分"></a>额外部分</h2><p>本文题目的这篇文章可以认为是作者对前一篇工作的改进，具体改进部分有两点：</p><h3 id="人工特征信息"><a href="#人工特征信息" class="headerlink" title="人工特征信息"></a>人工特征信息</h3><p>考虑到有些人工特征信息是十分重要的 ，但如果直接让模型去学习，需要花费很大的精力，现在整个神经网络结构也趋向于加入一些简单的人工特征信息，本文也是考虑到这些信息，具体如下图：</p><p><img src="/2018/06/20/2018-06-20/2018-06-20-3.PNG" alt="hand-crafted features"></p><p>这些特征基本上将entity mention的一些语义信息，主题信息等考虑了进去，相当于增加了很多先验知识，这个还是十分有用的，那如何加这些信息呢？作者首先用0，1向量表示这些特征信息，然后将其映射到低维空间，最后得到人工特征的向量表示</p><p>$$v_f = W_ff(m). \quad \tag{4.1}$$</p><p>然后将其加入到公式(3.1)中的拼接向量，变为如下形式：</p><p>$$y = \frac{1}{1 + exp(-W_y[v_m, v_c, v_f])}, \quad \tag{4.2}$$</p><p>这个还是很有特点的，一般我们都是将这些特征信息加入到最初的输入中，这样丰富了输入的信息，整个模型也可以更好地利用这些信息，这个方法个人感觉这些特征信息利用的不够充分。</p><h3 id="分层标签编码"><a href="#分层标签编码" class="headerlink" title="分层标签编码"></a>分层标签编码</h3><p>这是本文另一个很有意思的地方，首先，作为分类目标，其实考虑到他的类别表示，也是可以使用词向量进行编码的，而且这么做有一个好处，一些不常见的标签可以通过这种方式找到离他比较近的语义空间向量，从而能够更准确的进行分类，作者也是考虑到这样的信息，同时分类内容比较细的话，标签信息之间具有包含关系，因此，作者使用0，1向量表示类别信息，同时包含high-level和low-level的标签，然后对它们进行混合编码，而不是每个类别一个向量表示，具体可以从下图感受：</p><p><img src="/2018/06/20/2018-06-20/2018-06-20-4.PNG" alt="label encoding"></p><p>然后就得到了类别表示矩阵$W_y$，巧妙地地方在于，作者使用这个矩阵表示作为公式（4.2）分类层的参数，从而很好的将类别信息引入到了分类过程中，这样模型在分类的的时候就能获取更多的信息了，这个信息融合方式还是很有意思的。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>照例贴出最后的结果图，作者的方法虽然简单，但还是很有效的</p><p><img src="/2018/06/20/2018-06-20/2018-06-20-5.PNG" alt="part result"></p><h2 id="个人总结"><a href="#个人总结" class="headerlink" title="个人总结"></a>个人总结</h2><p>这篇文章展示了基于神经网络的NET模型的基本结构，包括对context的处理，对entity mention的处理，attention的使用，人工特征的添加等，是一篇很不错的文章，同时该方法也有很多地方值得改进，例如人工信息的利用，注意力机制，对文本信息的处理等，感觉还是有很多地方可以改进的，值得思考↖(^ω^)↗</p>]]></content>
      
      <categories>
          
          <category> paper_reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NET </tag>
            
            <tag> representation </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering</title>
      <link href="/2018/06/17/paper-reproduction/"/>
      <url>/2018/06/17/paper-reproduction/</url>
      <content type="html"><![CDATA[<blockquote><p>Title: Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering </p><p>Author: Wuwei Lan, Wei Xu </p><p>Link: <a href="https://arxiv.org/pdf/1806.04330" target="_blank" rel="noopener">https://arxiv.org/pdf/1806.04330</a></p></blockquote><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>这篇文章是COLING 2018的beat reproduction paper，文章主要对现有的做句子对任务的最好的几个模型进行了重现，并且作者实现出来的效果和原文章声称的效果相差不多，这点还是很厉害的，而且作者对语义理解的集中任务也做了相关梳理，文章简单易读，还是很值得一看的。</p><h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>句子对建模是NLP，NLU中比较基础，并扮演着重要角色的任务，主要集中在语义理解，语义交互上，也是我自己的一个研究方向，大致有这几类任务</p><ol><li>Semantic Textual Similarity (STS) ：判断两个句子的语义相似程度（measureing the degree of equivalence in the underlying semantics of paired snippets of text）</li><li>Natural Language Inference (NLI) ：也叫Recognizing Textual Entailment(RTE)，判断两个句子在语义上是否存在推断关系，相对任务一更复杂一些，不仅仅是考虑相似，而且也考虑了推理。</li><li>Paraphrase Identification (PI) ：判断两个句子是否表达同样的意思（identifing whether two sentences express the same meaning）</li><li>Question Answering (QA) ：主要是指选择出来最符合问题的答案，是在给定的答案中进行选择，而不是生成</li><li>Machine Comprehension (MC) ：判断一个句子和一个段落之间的关系，从大段落中找出存在答案的小段落，对比的两个内容更加复杂一些。</li></ol><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>有了任务，作者选取了集中目前情况下最好的模型，因为原文中每个模型可能只针对了某些任务进行了很多优化，那这些模型是否真的有效呢，作者考虑这些模型在所有的任务上进行比较，在介绍模型之前，作者首先介绍了句子对建模的一般框架：</p><h3 id="一般框架"><a href="#一般框架" class="headerlink" title="一般框架"></a>一般框架</h3><ol><li>输入层：适用预训练或者参与训练的词向量对输入中的每个词进行向量表示，比较有名的Word2Vec，GloVe，也可以使用子序列的方法，例如character-level embedding</li><li>情境编码层：将句子所处的情境信息编码表示，从而更好的理解目标句子的语义，常用的例如CNN, HighWay Network等，<strong>如果是句子语义表示的方法，一般到这里就结束了，接下来会根据具体的任务直接使用这一层得到语义表示</strong> </li><li>交互和注意力层：该层是可选的，句子语义表示有时候也会用到，但更多的是词匹配方法用到的，通过注意力机制建模两个句子在词层面的匹配对齐关系，从而在更细粒度上进行句子对建模，<strong>个人认为句子语义表示也会用到这些，只是句子语义表示最后会得到一个语义表示的向量，而词匹配的方法不一定得到句子语义的向量</strong></li><li>输出分类层：根据不同的任务，使用CNN，LSTM，MLP等进行分类判断。</li></ol><p>下图展示了一些句子语义表示的模型的基本框架：</p><p><img src="/2018/06/17/paper-reproduction/2018-06-17-3.PNG" alt="model structure"></p><p>有了这个一般的框架，接下来作者选取了集中目前最好的模型进行重现</p><h3 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h3><ol><li>InferSent[1]：BiLSTM+max-pooling</li><li>SSE[2]：如图1，和InferSent比较类似</li><li>DecAtt[3]：词匹配模型的代表，利用注意力机制得到句子1中的每个词和句子2中的所有词的紧密程度，然后用句子2中的所有词的隐层状态，做加权和表示句子1中的每个词</li><li>ESIM[4]：考虑了一些词本身的特征信息，和DecAtt比较类似</li><li>PWIM[5]：在得到每个词的隐层状态之后，通过不同的相似度计算方法得到词对之间相似关系，最后利用CNN进行分类。</li></ol><h2 id="数据："><a href="#数据：" class="headerlink" title="数据："></a>数据：</h2><p>为了更好的展示每个数据的情况，在这里直接用下图展示作者使用到的数据集：</p><p><img src="/2018/06/17/paper-reproduction/2018-06-17-6.PNG" alt="data info"></p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>直接上结果，上图是原文章中的结果，下图是作者重现的结果</p><p><img src="/2018/06/17/paper-reproduction/2018-06-17-4.PNG" alt="origin result"></p><p><img src="/2018/06/17/paper-reproduction/2018-06-17-5.PNG" alt="reproduction result"></p><p>从结果上看，作者实现的效果还是很厉害的，基本上跟原文章声明的不相上下，当然由于不是针对特定任务进行特别优化，所有效果还是有一点点差的，但基本上可以认为是实现了原来的效果，而且作者也发现了一些有意思的现象，例如：表现最好的就是ESIM，个人感觉这里面加入了很多次本身的一些信息，例如近义词，反义词，上下位信息等，这些信息其实对句子语义理解十分重要。</p><p>以上就是这篇文章的整体介绍，作者完整实现了这些方法，并在不同的数据集上进行验证，工作量还是很大的，而且对句子对建模进行了比较完整的介绍，还是很有意思的。♪(＾∀＾●)ﾉ</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>[1]：<a href="https://arxiv.org/pdf/1705.02364" target="_blank" rel="noopener">Supervised learning of universal sentence representations from natural language inference data </a></p><p>[2]：<a href="https://arxiv.org/pdf/1708.02312" target="_blank" rel="noopener">Shortcut-stacked sentence encoders for multi-domain inference </a></p><p>[3]：<a href="https://arxiv.org/pdf/1606.01933" target="_blank" rel="noopener">A decomposable attention model for natural language inference </a></p><p>[4]：<a href="http://www.aclweb.org/anthology/P17-1152" target="_blank" rel="noopener">Enhanced LSTM for natural language inference </a></p><p>[5]：<a href="http://www.aclweb.org/anthology/N16-1108" target="_blank" rel="noopener">Pairwise word interaction modeling with deep neural networks for semantic similarity measurement </a></p>]]></content>
      
      <categories>
          
          <category> paper_reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> natural language inference </tag>
            
            <tag> natural language understanding </tag>
            
            <tag> semantic similarity </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Improving Neural Fine-Grained Entity Typing with Knowledge Attention</title>
      <link href="/2018/06/17/paper-NET/"/>
      <url>/2018/06/17/paper-NET/</url>
      <content type="html"><![CDATA[<blockquote><p>Title: Improving Neural Fine-Grained Entity Typing with Knowledge Attention </p><p>Author: Ji Xin,  Yankai Lin, Zhiyuan Liu, Maosong Sun</p><p>Link: <a href="http://nlp.csai.tsinghua.edu.cn/~lzy/publications/aaai2018_entitytyping.pdf" target="_blank" rel="noopener">http://nlp.csai.tsinghua.edu.cn/~lzy/publications/aaai2018_entitytyping.pdf</a></p></blockquote><h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>本文针对的是Named Entity Typing(NET)任务，该任务主要是判断一个实体表示(entity mention)的类别，粒度的分类可以分为：person, location, organization，others。细粒度的分类根据不同的数据有着不同的分类情况，例如：FIGER dataset 有112类；也可以根据需要自定义类别个数进行分类，例如：从DBpedia中抽取22类。</p><p>该任务也是一个比较基础的任务，可以为问答，对话，语义理解等提供一些辅助理解的信息，而且和语义理解类似，同一个实体表示在不同的情境(context)下的类别是不一样，同样需要考虑情境信息。</p><h2 id="现状"><a href="#现状" class="headerlink" title="现状"></a>现状</h2><p>传统的方法大多集中在特征抽取，例如选取多种特征抽取的方法，例如pattern-based extractor，Knowledge base，mention-based extractor等，利用这些不同的方法得到实体表示的不同类别候选，然后利用词义消歧（word sense disambiguation  ）的方法选择最合适的类别。这里边最受关注的地方在于知识图谱的运用，知识图谱将很多先验知识及关系整合到图中，尤其是NET中的细粒度分类， 单纯利用神经网络来做是不行的，很多类别的样本都很少，而知识图谱可以提供必要的帮助。</p><p>神经网络的方法目前也慢慢多起来，神经网络在提取特征阶段已经被证明非常有效，因此在该任务上神经网络+知识图谱应该是一种非常有效的方法，而本文就是这样的一种方法。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="/2018/06/17/paper-NET/2018-06-17-1.PNG" alt="model structure"></p><p>模型整体看起来是比较简单的，而本文主要是为了解决NET中的两个问题：</p><ol><li>在实体表示分类时需要考虑其所存在的情境，而这点是被忽略的；</li><li>虽然知识图谱已经很早就被用到了NET中，但是知识图谱中实体之间的关系，并没有得到很好的利用。</li></ol><p>因此，作者提出了如上的模型，具体如下：</p><p>首先适用预训练的模型对所有的词进行标识 ，得到每个词的词向量，</p><h3 id="实体表示的向量表达"><a href="#实体表示的向量表达" class="headerlink" title="实体表示的向量表达"></a>实体表示的向量表达</h3><p>考虑到实体表示的短语是比较短的，大多是都是一个或者两个词，因此直接将每个词的向量相加，取均值，得到实体表示的向量表达：</p><p>$$m = \frac{1}{n_m}\sum_{i=1}^{n_m}m_i \quad \tag{1}$$</p><h3 id="情境信息的表示"><a href="#情境信息的表示" class="headerlink" title="情境信息的表示"></a>情境信息的表示</h3><p>为了更好的表示中心词的情境，例如中心词所存在的句子，作者使用BiLSTM分别整合左边和右边的情境信息，并利用注意力机制整合他们在LSTM中的隐层状态，最后整合为一个情境表示向量：</p><p>$$c = \frac{\sum_{i=1}^{L}(a_i^l[\overrightarrow{h_i^l}, \overleftarrow{h_i^l}]+a_i^r[\overrightarrow{h_i^r}, \overleftarrow{h_i^r}]) }{\sum_{i=1}^{L}a_i^l+a_i^r} \quad \tag{2}$$</p><p>这个公式还是很好理解的，将每个状态的两个方向表示拼接起来，乘以对应的权重，然后做加权和，最后再做平均，就得到了情境信息的向量表示，那么接下来就是如何表示情境信息的权重了。</p><h3 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h3><p>在这里作者考虑了很多不同的注意力机制，</p><ol><li><p>语义注意力机制，就是考虑情境信息的权重，由自身的语义信息来计算所占的比重，类似于self-attention：</p><p>$$a_i^{SA} = \sigma(W_{s1}tanh(W_{s2}[\overrightarrow{h}_i, \overleftarrow{h}_i])). \quad \tag{3.1}$$</p></li><li><p>因为我们需要考虑的是实体表示的类别，那么也可以计算实体表示所关注的重点在什么地方，也就是基础版的attention：</p><p>$$a_i^{MA} = \sigma(mtanh(W_{MA}[\overrightarrow{h}_i, \overleftarrow{h}_i])). \quad \tag{3.2}$$</p></li><li><p>正如之前问题里提到的那样，知识图谱不仅包含了实体，同时还有实体之间的关系信息，这点也是十分重要的，因此作者使用了知识图谱中表示关系的方法TransE来处理知识图谱中的关系表示，并将其应用到注意力机制中：</p><p>$$a_i^{KA} = \sigma(etanh(W_{KA}[\overrightarrow{h}_i, \overleftarrow{h}_i])), \quad \tag{3.3}$$</p><p>这其中$e$就是实体的embedding表示，</p></li></ol><h3 id="类别预测"><a href="#类别预测" class="headerlink" title="类别预测"></a>类别预测</h3><p>有了实体表示的向量和情境信息的向量，最后就是一个分类了，作者将得到的两个向量表示拼接起来，然后通过一个两层的MLP进行分类，得到最后的结果：</p><p>$$y = \sigma(W_{y1}tanh(W_{y2}[m, c])), \quad \tag{4}$$</p><p>最后使用二分类的交叉熵作为损失函数，因为每一个实体表示的类别可能不止一个，这可能是一个多分类问题，因此作者在每个类别上都是用了二分类的交叉熵，最后将所有的全都加起来作为最终的损失函数。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>照例，还是贴一个最后的结果图，因为使用了神经网络将情境信息融入到了实体分类中，可以更好地分类出在特定情境下的类别，也就更符合实际情况，从而获得更好的效果，当然将知识图谱中的关系信息也考虑进去也是本文的另一个创新点，这点还是很有意思的。</p><p><img src="/2018/06/17/paper-NET/2018-06-17-2.PNG" alt="result table"></p><h2 id="个人评价"><a href="#个人评价" class="headerlink" title="个人评价"></a>个人评价</h2><p>这是我接触NET之后读的第一篇利用神经网络来做的方法，对比那些利用不同的特征抽取方法来对实体表示进行分类，神经网络在抽取特征方面确实是省了不少的力气，而且整个模型简单有效，这就是一种非常好的方法。当然，该方法相对来说还是比较简单的，例如在情境信息的表示，情境信息和知识图谱的融合，相互辅助等方面还有很多可以做的，而且这个问题对语义理解也是一个很重要的方面，很值得学习研究。</p><p>以上就是这篇文章的整体介绍，NET还是很有意思的，搞起来搞起来♪(＾∀＾●)ﾉ</p>]]></content>
      
      <categories>
          
          <category> paper_reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NET </tag>
            
            <tag> KB </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Sentence Modeling via Multiple Word Embeddings and Multi-level Comparison for Semantic Textual Similarity</title>
      <link href="/2018/06/07/paper-20180607/"/>
      <url>/2018/06/07/paper-20180607/</url>
      <content type="html"><![CDATA[<p>这篇文章主要是集中在对词的多方面语义表示，在此基础上实现对句子语义的准确理解，并在多个句子语义理解相关任务上取得了不错的表现。</p><blockquote><p>Different word embedding models capture different aspects of linguistic properities.</p></blockquote><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>不同的词向量模型获取的是不同的语义信息，例如：BOW的上下文反映的是词的domain信息（scientist and research），基于解释关系的上下文反映的是词的语义相似度信息（boy and kid），因此作者提出来通过选取不同的上下文来获取词在不同方面的语义信息，具体作者选了</p><ol><li>word2vec：在Google News上进行训练的</li><li>fastText：在wikipedia上训练的</li><li>GloVe：选择的是300d的词向量，训练数据为Common Crawl</li><li>Baroni：使用的是context-predict的方法，使用的训练数据为English Wikipedia+British National Corpus</li><li>SL999：使用的训练数据为paraphrase database PPDB，然后再SimLex-999上微调</li></ol><p>以下是模型的整体框架图：</p><p><img src="/2018/06/07/paper-20180607/2018-06-07-1.PNG" alt="model-structure"></p><h3 id="输入处理"><a href="#输入处理" class="headerlink" title="输入处理"></a>输入处理</h3><p>作者使用K个预训练的词向量表示每个词，然后将这些结果拼接起来，作为每个词的最终表示</p><p>$$e_w^{concat} = e_w^1\oplus e_w^2 \oplus … \oplus e_w^K \tag{1}$$</p><h3 id="卷积网络"><a href="#卷积网络" class="headerlink" title="卷积网络"></a>卷积网络</h3><p>为了学习到一个可以表示每个词的multi-aspect的语义，作者使用了一个卷积网络的结构，有H个卷积核，每个卷积核可以通过一个线性变换表示，具体如下：</p><p>$$e_w^{r_i} = \sigma(e_w^{concat}r_i^T + b_{r_i}) \\ e_w^{multi} = [e_w^{r_1}, e_w^{r_2}, … , e_w^{r_H}] \tag{2}$$</p><h3 id="句子建模"><a href="#句子建模" class="headerlink" title="句子建模"></a>句子建模</h3><p>使用卷积网络的好处是参数共享，可以学习到局部特征，但是卷积神经网络有一个问题，在处理序列数据的时候容易丢失序列信息，因此作者在这里使用了LSTM来处理，从而保证了序列信息的保留，具体如下，作者首先在句子中所有词的相同纬度上进行max-pooling，获取到最有价值的信息，同时通过LSTM实现对整个句子的建模，然后选取最后一个状态作为句子的语义向量表示，最后将max-pooling的结果和LSTM的最后一个状态进行拼接，从而得到句子的语义表示，具体如下：</p><p>$$e_s^{max}[i] = max(e_{w_1}^{multi}[i], e_{w_2}^{multi}[i], …, e_{w_n}^{multi}[i]) \\ e_s^{lstm} = LSTM(e_w^{multi})[-1] \\ e_s = e_s^{max} \oplus e_s^{lstm} \tag{3}$$</p><h3 id="多尺度比较"><a href="#多尺度比较" class="headerlink" title="多尺度比较"></a>多尺度比较</h3><p>有了每个句子的语义表示，根据任务的不同，就可以进行比较了，作者在这里选取了三种比较方式</p><ol><li>word-word similarity：</li></ol><p>$$A_{ij} = \frac{s_1^{multi}[i] \cdot s_2^{multi}[j]}{\left|s_1^{multi}[i]\right|\left|s_2^{multi}[j]\right|} \\ sim^{word} =  \sigma(W^{word}g(A)+b^{word})\tag{4}$$</p><p>这个公式中，g表示的是讲一个矩阵展平为向量的函数，这是词与词之间的相似度比较</p><ol start="2"><li><p>sentence-sentence comparison:</p><ol><li>Cosine Similarity:</li></ol><p>$$d_{cosine}=\frac{e_{s_1}\cdot e_{s_2}}{\left|e_{s_1}\right|\left|e_{s_2}\right|} \tag{5.1}$$</p><ol start="2"><li>Multiplication vector &amp; Absolute difference:</li></ol><p>$$d_{mul}=e_{s_1}\odot e_{s_2} \\ d_{abs} = |e_{s_1} - e_{s_2}| \tag{5.2}$$</p><ol start="3"><li>Neural difference:</li></ol><p>$$x = e_{s_1}\oplus e_{s_2} \\ d_{neu} = W^{neu}x + b^{neu} \tag{5.3}$$</p><p>最后将这些不同的结果拼接起来，做一个线性变换</p><p>$$d^{sent} = d_{cosine}\oplus d_{mul}\oplus d_{abs} \oplus d_{neu} \\ sim^{sent} = \sigma(W^{sent}d^{sent} + b^{sent}) \tag{5.4}$$</p></li><li><p>word-sentence comparison:</p></li></ol><p>在这部分就是让句子1的语义表示和句子2的每个词的语义表示进行比较，然后将比较结果矩阵展平，通过线性变化，得到这一阶段的相似度值</p><p>$$e_{s_1}^{ws}[i] = e_{s_1} \oplus s_2^{multi}[i] \\ sim_{s_1}^{ws}[i] = \sigma(W^{ws}e_{s_1}^{ws}[i]+b^{ws}) \\ sim^{ws}=\sigma(W^{ws^{‘}}[g(sim_{s_1}^{ws})\oplus g(sim_{s_2}^{ws})] + b^{ws^{‘}}) \tag{6}$$</p><h3 id="最后进行分类"><a href="#最后进行分类" class="headerlink" title="最后进行分类"></a>最后进行分类</h3><p>在这一阶段，作者将三种不同的相似度信息进行拼接，然后通过线性变换，最后softmax进行分类</p><p>$$sim = sim^{word} \oplus sim^{sent} \oplus sim^{ws} \\ h_s = \sigma(W^{l1}sim+b^{l1}) \\ \hat{y} = softmax(w^{l2}h_s+b^{l2}) \tag{7}$$</p><p>最后的输出根据不同任务的不同有不同的修改。以上就是整个模型的过程。</p><h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>作者是在比较两个句子之间的相似度，因此作者在不同的几个任务上进行了比较，分别是1）RTE: SICK数据集；2）STS：STSB和SICK数据集，这部分需要吐槽的一点是在RTE任务上居然没有使用SNLI这个大规模数据集，可能是效果表现不好，个人感觉</p><h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><p>这里就贴一张结果图来展示作者实验的效果吧</p><p><img src="/2018/06/07/paper-20180607/2018-06-07-2.PNG" alt="results"></p><h2 id="个人评价"><a href="#个人评价" class="headerlink" title="个人评价"></a>个人评价</h2><p>首先作者的这个idea还是很有意思的，我在16年的一篇工作也使用了同样的一个idea，基本的motivation也是相同的，只是他在多个不同的任务上进行验证，同时将多方面的词表示扩展到了句子的语义表示上，我那个工作只是在词级别上的一个分析，同时只考虑了Recognizing lexical entailment这个任务，这点是我的不足。但这篇文章在句子语义建模上有些简单，因此个人感觉他的句子语义表示向量的效果可能不是很好，但作者在最后的相似度上考虑了词对词，词对句子，句子对句子三个方面，这个还是很有意思的，这点值得学习一下，考虑多方面语义时不仅要考虑输入上的多方面，在比较上也可以进行多方面的比较。</p><p>以上就是这篇文章的整体介绍，介绍完感觉有必要介绍一下我自己的工作，下次介绍♪(＾∀＾●)ﾉ</p>]]></content>
      
      <categories>
          
          <category> paper_reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> word semantic </tag>
            
            <tag> natural language understanding </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>论文投稿有感</title>
      <link href="/2018/06/06/After-Writing/"/>
      <url>/2018/06/06/After-Writing/</url>
      <content type="html"><![CDATA[<p>刚刚忙完了论文的投稿，虽说也写了几篇文章，但是进入依然十分缓慢，这次记录下自己在这次投稿过程中的感想，与君共勉</p><h2 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h2><p>标题是整篇文章的开头，一定要清晰直接的介绍出自己的工作的卖点，不能使用那些会造成误解的词，否则开头就容易把人带偏，就像我这次，开始使用的是<strong>Hierarchical</strong>，后来改成了<strong>Multi-Level</strong>，虽然前者也有后者的意思，但是前者更偏向于树形结构，有等级从属关系，这样就容易给人造成误解， 所以起一个好的名字是十分有必要的。</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>摘要是一篇文章最精炼的部分了，从我的感觉来看，可以分成以下的形式：</p><ul><li>第一句话：介绍要研究的问题</li><li>第二句话：当前工作有什么难题</li><li>第三句话：我们提出了什么方法，直接点明</li><li>第四句话：我们方法的卖点</li><li>第五句话：效果是怎么样的，如果有具体数据支撑就更好了</li></ul><p>基本上可以用这些来表示，需要注意的是在这里提出的问题，突出的卖点是整篇文章的核心，在之后的部分都要有具体的支撑。摘要部分是最精炼的部分，也展示了文章最突出的地方，能让读者看了之后就明白作者的想法。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>这部分我先举一个我在整个文章过程中的修改历程：</p><ol><li>介绍问题-&gt;当前的研究现状-&gt;问题1，我们需要怎么做-&gt;正好有一些别的任务证明这么做是有效的-&gt;问题2，我们需要怎么做-&gt;总结，提出我们的方法，简单介绍-&gt;写贡献点</li><li>介绍问题-&gt;分类介绍当前研究现状及特点-&gt;存在的问题2，我们应该怎么做-&gt;问题1，我们应该怎么做-&gt;正好有一些别的任务证明这么做有效&gt;总结，提出我们的方法，简单介绍-&gt;写贡献点</li><li>介绍问题-&gt;分类介绍当前研究现状及特点-&gt;问题1，我们需要怎么做-&gt;当我们这么做时又会碰到什么问题-&gt;问题2，需要解决-&gt;基于以上观点，提出我们的方法，简单介绍-&gt;写贡献点</li></ol><p>以上就是我在introduction上的大版本修改过程，因为我最初想突出了两个贡献点，而这两个贡献点彼此相差比较大，强硬写到一起，只会逻辑不通，读者也不知道你要表达什么，因此，突出一个重点，围绕这个重点写，所有的工作都是围绕这个重点展开的，这样就会十分清晰，也就容易看懂了。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related-Work"></a>Related-Work</h2><p>这部分好写，只需要将相关的工作做一介绍，整合他们的特点，指出他们存在的不足即可，需要注意的是相关工作一定要全面，不能丢三落四，否则很容易体现出你的相关调研做的不充分。</p><p>但这部分也很难写，如何使用简短的语句将当前的研究现状做一梳理，整合他们的特点，在此基础上提出新的论点，这点其实是非常考验作者功力的，我也在朝这个方向努力中。</p><p>还有需要强调一点的，这部分最好全都用一般过去时</p><h2 id="Problem-Statement-amp-Technical-Details"><a href="#Problem-Statement-amp-Technical-Details" class="headerlink" title="Problem Statement &amp; Technical Details"></a>Problem Statement &amp; Technical Details</h2><p>首先是问题形式化，这部分相对简单，但要注意符号的运用；同时可以在这里指出在问题中我们要解决什么问题，这个问题和我们的卖点是对应的，这样可以告诉读者，接下来需要注意那些地方，读者也能有重点的看这部分。</p><p>接着是技术部分，几点经验教训吧：</p><ol><li>符号的定义，最好可以定义一个符号表，保证符号使用规范以及不出错，相关规范可以参考Ian Goodfellow的《深度学习》这本书，同时要注意符号要前后照应，和模型图对应，和各种分析对应；如果使用latex的话，可以定义一个newcommand，这样在后续修改过程中只需要改动这个地方就可以了 ，类似于程序中的头文件。</li><li>不能像实验报告那样，只写我们做了什么，我们怎么做，这是绝对不行的，这个也是我要注意的问题</li><li>不能像技术文档那样，事无巨细，每一条都有解释，有引文，这样就太啰嗦了，也是不行的</li><li>技术部分要详略得当，一些众所周知的内容，可以简单写，涉及到自己工作卖点的部分，一定要清晰解释，为什么要这么做，这么做有什么好处，这么做可以达到一个什么样的效果，然后是具体的做法，这点非常非常重要。</li><li>写公式的时候最好可以按照数学的方式写，作为一个计算机专业的学生，有时候写着写着就习惯于按照写程序的方式来写了，须知在程序中的相乘方式和在数学公式中是不一样的，这点需要十分注意。</li><li>模型图也要根据自己的卖点突出相应的重点，可以让读者不用看文字就能知道大致的重点是什么，要突出的地方要重点展示出来，同时图中的符号要与文字部分保持一致，保证能够彼此印证，最后图要画的美观大方，不能太繁琐，太复杂。</li></ol><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>这部分我感觉是我的薄弱部分，因为有的时候不知道要做哪些实验来验证，写一个大致的想法吧，首先肯定是当前任务上的整体效果，这点一定要好，否则你就无法体现你模型的贡献了，在这部分，要注意baseline的选择，能够体现出自己工作的特点，同时能够展现当前的state-of-the-art效果，baseline的选择也是一门艺术。其次对自己模型的分析 ，既然提出了某个点十分重要，那肯定需要通过实验来验证它的重要性，并于之前的部分相呼应，因此ablation performance就十分有必要了。接着就是一些参数敏感性实验之类的，总之这部分要十分充实。同时在图标上，表格，折线图，柱状图，饼图等多样化来展现模型的特点，同时也体现出专业性。另外，如果可以做一个case study的话，最好做一个，这样可以更直观的体现出模型的卖点。</p><p>也有一点需要强调的，方便书写，这部分用一般现在时会好一些。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>最后这部分我一般都是做一简单总结，以及对未来工作的一些简单想法，目前还没有学习到好的conclusion写法，还在努力学习中。</p><p>还有就是一些小问题了，例如语法，时态，拼写等，这些会经常出现在文章中，因此仔细写，多检查，细心一些是十分有必要的。</p><p>以上，就是此次投文章的一些经验总结，希望下次能写出更好的文章出来，↖(^ω^)↗</p>]]></content>
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper-writing </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>自然语言推理介绍</title>
      <link href="/2018/05/28/NLI-introduce/"/>
      <url>/2018/05/28/NLI-introduce/</url>
      <content type="html"><![CDATA[<p>自然语言推理作为自然语言理解的一个重要组成部分，在整个自然语言理解中扮演着重要的角色，接下里我将对自然语言推理的现状做一简单总结，以下内容是我的小组分享的记录版。</p><h2 id="自然语言推理简介"><a href="#自然语言推理简介" class="headerlink" title="自然语言推理简介"></a>自然语言推理简介</h2><p><img src="/2018/05/28/NLI-introduce/2018-05-28-1.PNG" alt="Natural Language Inference"></p><p>自然语言推理主要是判断两个句子（Premise, Hypothesis）或者两个词之间的语义关系，为了保证模型能够集中在语义理解上，该任务最终退化为一个分类任务，目前类别主要是三分类（Entailment，Contradiction，Neutral）。目前对这三类有各种各样的定义，但是我认为这三类的分类依据还是要落在语义理解上，通过语义关系来确定类别。</p><p><img src="/2018/05/28/NLI-introduce/2018-05-28-2.PNG" alt="The importance of NLI"></p><p>那为什么要研究自然语言推理呢？简单来讲，机器学习的整个系统可以分为两块，输入，输出。输入要求我们能够输入一个机器能理解的东西，并且能够很好的表现出数据的特点，输出就是根据需要，生成我们需要的结果。也可以说整个机器学习可以分为Input Representation和Output Generation。因此，如何全面的表示输入就变得非常重要了。而自然语言推理是一个分类任务，使用准确率就可以客观有效的评价模型的好坏；这样我们就可以专注于语义理解和语义表示。并且如果这部分做得好的话，例如可以生成很好的句子表示的向量，那么我们就可以将这部分成果轻易迁移到其他任务中，例如对话，问答等。这一切都说明了研究自然语言推理是一个非常重要但是非常有意义的事情。</p><p>以下是自然语言推理推理的发展历程</p><p><img src="/2018/05/28/NLI-introduce/2018-05-28-3.PNG" alt="Path of NLI"></p><h2 id="相关数据"><a href="#相关数据" class="headerlink" title="相关数据"></a>相关数据</h2><p>上一部分对自然语言推理进行了一个大致的介绍。众所周知，数据对模型非常重要，而且深度神经网络也是高度依赖数据的，那么我们来对目前的数据进行一个简单的梳理</p><p><img src="/2018/05/28/NLI-introduce/2018-05-28-4.PNG" alt="SNLI"></p><p>这是自然语言推理领域一个非常重要的数据集，相关信息如上图所示，斯坦福大学通过众包的方式生成了这个自然语言推理领域第一个大规模人工标注的数据集，从此自然语言推理进入深度学习时代。</p><p><img src="/2018/05/28/NLI-introduce/2018-05-28-5.PNG" alt="Multi_NLI"></p><p>这个数据集和SNLI比较类似，但是它的前提句子来自真实场景的数据，因此在数据上更贴近现实一些，同时，每一条数据包含了类别信息。</p><p><img src="/2018/05/28/NLI-introduce/2018-05-28-7.PNG" alt="SciTail"></p><p>这个数据集就更加贴近现实了，前提句和假设句全都来自真实场景，人工的作用放到了对每一条数据打标签，因此数据本身的人工影响就变小了。但是该数据全部来自科学问答，因此在类别上可能略显单一。</p><p><img src="/2018/05/28/NLI-introduce/2018-05-28-6.PNG" alt="MPE"></p><p>该数据集和前边不同的地方在于它是四个前提句对应一个假设句，这样在进行推理分类时，不仅需要考虑前提句和假设句的关系，还需要考虑前提句之间的相关关系，因此推理难度更大，但是这些文本句子全都来自image captioning工作，文本本身人工较少，因此文本质量上可能不如前者。但这也是一个很有意思的方向。</p><p>当然，还有很多其他的数据集，例如SICK, Add-one RTE, JOCI等，都是从不同角度对语义理解提出了一个要求。可以说自然语言推理领域目前正是百花齐放，十分繁荣。</p><h2 id="一些方法"><a href="#一些方法" class="headerlink" title="一些方法"></a>一些方法</h2><h3 id="词级别的推理"><a href="#词级别的推理" class="headerlink" title="词级别的推理"></a>词级别的推理</h3><p><img src="/2018/05/28/NLI-introduce/2018-05-28-8.PNG" alt="CENN"></p><p>该方法主要研究的是词级别的推理。有研究表明，词具有不同方面的语义信息，例如book在名词角度可以表示为：书，但在动词角度可以表示为：预定；因此使用单一的向量可能无法有效区分这些内容，因此作者提出利用不同的上下文来获取词在不同角度的语义信息，例如：选取中心词周围的名词来表示它的topic信息，选取中心词周围的动词来表示它的function信息。这样对每个词就都有不同的语义向量表示，然后通过网络结构对相关信息进行拼接，最后考虑到不同的推理关系可能需要的信息时不同的，例如：上下位关系：狗-动物，可能需要的是topic相关的信息，因果关系：攻击-受伤，可能需要的更多的是function的信息，因此作者通过一个门结构计算出每种语义表示对推理关系的影响程度，然后进行加权求和，最后进行分类。并且该方法的可扩展性非常好，从网络结构上看，我们可以增加不同的语义表示，模型的结构和参数规模并不会有太大的提升，这也可以认为是模型的一个优点。</p><h3 id="句子级别的推理"><a href="#句子级别的推理" class="headerlink" title="句子级别的推理"></a>句子级别的推理</h3><p>句子级别的推理可以分为两部分，基于句子语义编码的方法和基于词匹配的方法，前者是首先将一个句子编码为向量，然后分析两个向量之间的关系；后者考虑更多的是词之间的匹配关系，不一定有句子的语义向量表示。</p><h4 id="句子语义编码方法"><a href="#句子语义编码方法" class="headerlink" title="句子语义编码方法"></a>句子语义编码方法</h4><p><img src="/2018/05/28/NLI-introduce/2018-05-28-11.PNG" alt="Sentence-encoding"></p><p>该方法就是句子编码的方法，首先左图展现了这类方法的一般结构，首先通过不同的方法得到每个句子的语义向量表示；在此基础上，对两个向量作拼接，相减，点乘来得到两个语义向量之间的关系，最后通过一个MLP进行分类，右图就是句子编码部分可以采用的方法，例如：通过双向LSTM，得到隐层状态，对隐层状态做max-pooling或者做attention，得到的加权表示就只句子的语义向量表示，最右边的图示利用了CNN的结构，我们都知道CNN是建立输入之间的局部关系，那么作者使用了多层的CNN，通过多层卷积，底层获取的是local部分的信息，那么越往上就可以得到更长范围内的信息，从而对句子语义进行建模，这也是一种很不错的方法。</p><p><img src="/2018/05/28/NLI-introduce/2018-05-28-10.PNG" alt="Inner-atten"></p><p>这个方法是对注意力机制（Attention Mechanism）的一种有效利用，我们可以清晰看出来，作者先对BiLSTM的隐层状态进行mean pooling，在此基础上，利用注意力机制得到句子中那些词对语义表示比较重要，然后对隐层状态进行加权求和，就得到了句子表示的向量，最后就是常用的框架。从该方法中我们也可以看到句子编码模型的基本结构。</p><h4 id="词匹配方法"><a href="#词匹配方法" class="headerlink" title="词匹配方法"></a>词匹配方法</h4><p> <img src="/2018/05/28/NLI-introduce/2018-05-28-12.PNG" alt="word-matching"></p><p>该方法是词匹配方法的一个代表工作，首先作者使用两个LSTM来处理两个句子，并且后一个LSTM的隐层使用前一个LSTM的最后一个状态初始化，在此基础上，作者在求后一个句子的隐层状态时，使用注意力机制考虑前提句子的每一个词的隐层状态，建立词之间的匹配关系，最后利用最后一个隐层状态作为最后分类的依据，在这个方法中我们看到，其实并没有句子语义向量的表示。</p><p><img src="/2018/05/28/NLI-introduce/2018-05-28-13.PNG" alt="word-matching1"></p><p>该方法在词匹配上进行了更深入的研究，不仅仅是计算他们的匹配程度，而是先求前提句中的每个词和假设句中的所有词之间的attention，并使用假设句中所有词乘以attention分布，来表示前提句中的每个词，这样就使用了对方的语义来表示自己，假设句也是一样。然后将得到的结果和原始的表示作拼接，通过变换，最后求一个和，得到句子的表示进行分类，该方法也是目前比较流行的方法，通过使用对方的语义来表示自己，从而对语义关系进行更好的建模。</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h3><p><img src="/2018/05/28/NLI-introduce/2018-05-28-14.PNG" alt="accuracy"></p><p>这是第一个也是很重要的问题，目前在SNLI数据集上，最好的结果已经做到89.3%，那么接下来如何提升准确率呢？这是一个值得思考的问题。</p><h3 id="Lexical-Knowledge"><a href="#Lexical-Knowledge" class="headerlink" title="Lexical Knowledge"></a>Lexical Knowledge</h3><p><img src="/2018/05/28/NLI-introduce/2018-05-28-15.PNG" alt="lexical-knowledge"></p><p>这个问题很有意思，从数据上考虑，他只是修改了前提句中的一个词得到假设句，对于我们人类来说，进行这样的区分十分容易，但是由于两个句子之间的词的高重合度，模型可能会认为这两个输入是一致的，尤其是不同的两个词属于同一类的时候，他们的词向量表示会更相似。如果数据中有这样的例子，那模型肯定没问题，但如果训练数据中没有这样的例子，但实际上如果模型能够很好地理解语义，这个应该不是问题，而事实上这对模型来说是一个巨大的问题，对于这些词级别的不同，模型该如何去衡量呢？</p><h3 id="Annotation-Artifacts"><a href="#Annotation-Artifacts" class="headerlink" title="Annotation Artifacts"></a>Annotation Artifacts</h3><p><img src="/2018/05/28/NLI-introduce/2018-05-28-16.PNG" alt="annotation-artifacts"></p><p>这也是一个很有意思的问题，如前边所介绍的，数据集都是使用了人工，尤其是有一些数据集的假设句子全都是人写的，理论上来说人写的句子肯定比模型生成的好，但是人写的句子也有一些特点，例如推理关系是Entailment的时候，可能假设句的一些名词是前提句的上位词（woman-&gt;people），如果是Contradiction的时候，那么假设句中可能就有很多否定词之类的。这些特点其实很好理解，但是如果模型发现了这些特征，那么它甚至可以只用假设句就能进行分类，但结果正确并没有什么用，模型并没有真正理解语义。如图作者进行了一些统计分析，可以看到有一些词和标签是有着紧密联系了，可以直接用这些词进行分类，但这些对语义理解并没有什么帮助。因此如何避免这些情况，准确理解语义也是一个非常重要的研究内容。</p><h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>以上就是我针对自然语言推理的一个简单介绍，作为自然语言理解的一个重要组成部分，这里边还是有很多很有意思的内容值得研究的。♪(＾∀＾●)ﾉ</p>]]></content>
      
      <categories>
          
          <category> notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> natural language inference </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hexo+github搭建个人博客记录</title>
      <link href="/2018/05/26/blog-start/"/>
      <url>/2018/05/26/blog-start/</url>
      <content type="html"><![CDATA[<p>作为一个工科生，但一直希望能够记录一些东西；同样作为一个工科生，有能力自己搭建博客。看了很多大神的github博客，最终决定也用hexo+github搭建一个自己的个人小站，记录下自己的一些足迹。</p><h1 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h1><h2 id="github建立repository"><a href="#github建立repository" class="headerlink" title="github建立repository"></a>github建立repository</h2><p>在自己的github上新建repository，要注意的是name必须是username.github.io，该地址也是博客的地址，换成其他名字是不行的</p><h2 id="hexo安装"><a href="#hexo安装" class="headerlink" title="hexo安装"></a>hexo安装</h2><p>安装git和node.js</p><blockquote><p>git地址：<a href="https://git-scm.com/download/win" target="_blank" rel="noopener">https://git-scm.com/download/win</a></p><p>node.js地址：<a href="https://nodejs.org/zh-cn/" target="_blank" rel="noopener">https://nodejs.org/zh-cn/</a></p></blockquote><p>安装Hexo</p><blockquote><p>npm install -g hexo #-g表示全局安装, npm默认为当前项目安装</p></blockquote><h2 id="Hexo部署"><a href="#Hexo部署" class="headerlink" title="Hexo部署"></a>Hexo部署</h2><p>建立博客的根目录，（建议使用全英文目录，避免出现奇奇怪怪的bug），然后在该目录下打开git bash，（这里同样建议使用git bash，不使用windows terminal，因为windows terminal有一些log信息不会输出，这样就看不到哪里有问题了）</p><blockquote><p>hexo init #新建博客目录<br>hexo g #根据当前目录下文件生成静态网页<br>hexo s #启动服务器，还可以使用hexo s -p 4321，解决端口占用问题</p></blockquote><p>这样就可以在浏览器中输入localhost:4000查看了，</p><p>简单介绍一下文件目录，摘自<a href="http://www.shuang0420.com/2016/05/12/Github-Pages-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">这里</a></p><ul><li>public：执行hexo generate命令，输出的静态网页内容目录</li><li>scaffolds：layout模板文件目录，其中的md文件可以添加编辑</li><li>scripts：扩展脚本目录，这里可以自定义一些javascript脚本</li><li>source：文章源码目录，该目录下的markdown和html文件均会被hexo处理。该页面对应repo的根目录，404文件、favicon.ico文件，CNAME文件等都应该放这里，该目录下可新建页面目录。</li><li>drafts：草稿文章</li><li>posts：发布文章themes：主题文件目录</li><li>config.yml：全局配置文件，大多数的设置都在这里</li><li>package.json：应用程序数据，指明hexo的版本等信息，类似于一般软件中的 关于 按钮</li></ul><h2 id="Hexo-写文章"><a href="#Hexo-写文章" class="headerlink" title="Hexo 写文章"></a>Hexo 写文章</h2><blockquote><p> hexo new “postname” #然后在posts目录下的postname.md文件中编辑博客</p></blockquote><p>然后在source目录下打开对应的markdown文件，编辑即可，这里建议使用<a href="http://www.typora.io" target="_blank" rel="noopener">typora</a></p><h2 id="Hexo本地调试"><a href="#Hexo本地调试" class="headerlink" title="Hexo本地调试"></a>Hexo本地调试</h2><blockquote><p>hexo clean #清除之前生成的内容，保证不出问题<br>hexo g #生成<br>hexo s #启动本地服务，进行文章预览调试，也可以使用hexo s -p 4321</p></blockquote><h2 id="Hexo部署到github上"><a href="#Hexo部署到github上" class="headerlink" title="Hexo部署到github上"></a>Hexo部署到github上</h2><p>首先安装一个插件，保证能够使Hexo部署到GitHub上：</p><blockquote><p>npm install hexo-deployer-git –save</p></blockquote><p>在博客目录下找到配置文件_config.yml，进行编辑</p><p>编辑前：</p><blockquote><p># Deployment<br>##  Docs: <a href="http://hexo.io/docs/deployment.html" target="_blank" rel="noopener">http://hexo.io/docs/deployment.html</a><br>deploy:<br>  type:</p></blockquote><p>编辑后：</p><blockquote><p>deploy:<br>  type: git<br>  repo: 对应仓库的SSH地址（可以在GitHub对应的仓库中复制）<br>  branch: 分支（User Pages为master，Project Pages为gh-pages）</p></blockquote><p>然后执行：</p><blockquote><p>hexo g<br>hexo deploy</p></blockquote><p>之后就可以在浏览器中通过username.github.io进行浏览，棒棒哒</p><h2 id="优化部署和管理"><a href="#优化部署和管理" class="headerlink" title="优化部署和管理"></a>优化部署和管理</h2><p>虽然目前已经基本搭建好了博客，但是编辑只能在当前电脑编辑，github上保存的是生成之后的html文件，整个博客的源代码都在本地，那如果想要在别的电脑上编辑怎么办，这时候就要利用到github的分支，即在建立博客仓库的时候，建立两个分支，一个用于展示网站内容，一个用于存放hexo文件，</p><p>具体流程参考了<a href="http://crazymilk.github.io/2015/12/28/GitHub-Pages-Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/#more" target="_blank" rel="noopener">这里</a></p><h3 id="创建流程"><a href="#创建流程" class="headerlink" title="创建流程"></a>创建流程</h3><ol><li>创建仓库，username.github.io；</li><li>创建两个分支：master 与 hexo；</li><li>设置hexo为默认分支（因为我们只需要手动管理这个分支上的Hexo网站文件）；</li><li>使用git clone <a href="mailto:git@github.com" target="_blank" rel="noopener">git@github.com</a>:username/username.github.io.git拷贝仓库；</li><li>在本地username.github.io文件夹下通过Git bash依次执行npm install hexo、hexo init、npm install 和 npm install hexo-deployer-git（此时当前分支应显示为hexo）;</li><li>修改_config.yml中的deploy参数，分支应为master；</li><li>依次执行git add .、git commit -m “…”、git push origin hexo提交网站相关的文件；</li><li>执行hexo generate -d生成网站并部署到GitHub上。</li></ol><p>这样一来，在GitHub上的CrazyMilk.github.io仓库就有两个分支，一个hexo分支用来存放网站的原始文件，一个master分支用来存放生成的静态网页。</p><h3 id="日常修改"><a href="#日常修改" class="headerlink" title="日常修改"></a>日常修改</h3><p>在本地对博客进行修改，一般是如下步骤</p><ol><li>git pull  (在保证本地没有修改的情况下，更新到github上的版本，保持版本一致，非常重要)</li><li>进行各种编辑，修改操作</li><li>依次执行git add .、git commit -m “…”、git push origin hexo指令将改动推送到GitHub（此时当前分支应为hexo）</li><li>然后才执行hexo generate -d发布网站到master分支上</li></ol><h3 id="异地修改"><a href="#异地修改" class="headerlink" title="异地修改"></a>异地修改</h3><p>当在不同的电脑上修改时，一般是如下步骤</p><ol><li>使用git clone <a href="mailto:git@github.com" target="_blank" rel="noopener">git@github.com</a>:username/username.github.io.git拷贝仓库（默认分支为hexo）</li><li>在本地新拷贝的username.github.io文件夹下通过Git bash依次执行下列指令：npm install hexo、npm install、npm install hexo-deployer-git</li><li>然后就是日常修改中的2以后的操作</li></ol><h1 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h1><p>果然自己搭，坑不是一般的多，后续会慢慢更新，记录我在使用过程中踩过的坑↖(^ω^)↗</p>]]></content>
      
      <categories>
          
          <category> notes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hello World</title>
      <link href="/2018/01/01/hello-world/"/>
      <url>/2018/01/01/hello-world/</url>
      <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>. test2</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
        <tags>
            
            <tag> others </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
